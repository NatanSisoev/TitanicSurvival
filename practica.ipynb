{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pràctica 1: Resolem un problema de classificació\n",
    "\n",
    "## Objectius\n",
    "\n",
    "Els objectius d'aquesta pràctica són:\n",
    "\n",
    "* Aplicar els coneixements adquirits sobre processament de dades, classificació i validacio creuada.\n",
    "  \n",
    "* Ser capaç de comparar diferents models de classificació.\n",
    "\n",
    "* Ser capac de fer cerca d'hiperparàmetres.\n",
    "\n",
    "* Entendre i implementar la validació creuada.\n",
    "\n",
    "* Analitzar detalladament els resultats obtinguts durant l'aprenentatge dels diferents models.\n",
    "\n",
    "Aquesta pràctica és prèvia al cas kaggle que realitzareu durant la segona part de l'assignatura. En aquesta primera pràctica les preguntes estan definides, però us ha de servir d'aprenentatge a l'hora de saber com estructurar un projecte d'aprenentatge automàtic ja que en el cas kaggle no tindreu les preguntes.\n",
    "\n",
    "## Base de dades\n",
    "\n",
    "En aquesta pràctica farem servir la base de dades del titanic. L'atribut que predirem es Survived, el qual ens diu si cada passatger va sobreviure o no.\n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "\n",
    "## Treball en grup\n",
    "Aquesta pràctica es treballarà en grups de 2-3 persones. En casos excepcionals i degudament justificats la pràctica es podrà realitzar de forma individual.\n",
    "\n",
    "## Seguiment i entrega de la pràctica\n",
    "\n",
    "En la pràctica 1 es presenten diverses tasques per fer una correcta comparativa dels resultats obtinguts per diversos mètodes de classificació en una mateixa base de dades.\n",
    "\n",
    "En aquesta pràctica es realitzaran sessions de seguiment del treball. Aquestes sessions de treball estan orientades a que els alumnes que vingueu pugueu preguntar i resoldre dubtes sobre les dades, preguntar sobre l'objectiu de cada apartat dels enunciats que no us hagi quedat clar, i preguntar sobre els resultats que esteu obtenint a l'hora d'analitzar les dades. És molt recomanable venir a classe amb el treball fet per tal de poder comentar dubtes.\n",
    "\n",
    "Pel que fa l'entrega, caldrà entregar per caronte el següent:\n",
    "\n",
    "1. Memòria en format PDF explicant els resultats trobats sobre la bases de dades. La memòria ha d'utilitzar la plantilla de LaTeX que podeu trobar al Caronte i ha de ser de com a màxim 3 pàgines.\n",
    "   \n",
    "2. Notebook amb el respectiu codi de python.\n",
    "\n",
    "3. (Opcional) Presentació amb els resultats 4 min màxim."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Descripció de la pràctica\n",
    "\n",
    "A continuació es mostren tots els continguts que s'evaluaran en la pràctica:\n",
    "\n",
    "1. EDA (exploratory data analysis) (1 punt):\n",
    "  * Anàlisi de tamany i tipologia de dades\n",
    "  * Primera valoració de correlacions\n",
    "  * Anàlisi atribut target\n",
    "2. Preprocessing (2 punts):\n",
    "  * Eliminació de nans\n",
    "  * Encoding de categòriques\n",
    "  * Altres (PCA, normalització, ...)\n",
    "3. Metric selection (1.5 punts):\n",
    "  * Selecció de la millor mètrica pel problema\n",
    "  * Visualització de ROC/AUC per model base\n",
    "4. Model Selection amb Crossvalidation (4 punts):\n",
    "  * Selecció del millor model\n",
    "  * Cerca d'hiperparàmetres\n",
    "5. Anàlisi final (1.5 punt)\n",
    "\n",
    "La pràctica esta construida a partir d'un seguit de preguntes orientatives en cada apartat les quals tenen relació amb els continguts evaluables. **NO cal contestar-les totes**. Són una guia per a que reflexioneu i aprengueu detalls de cada apartat. És recomanable llegir totes les preguntes abans de realitzar la pràctica i tenir-les en ment a l'hora d'executar-la.\n",
    "\n",
    "\n",
    "**IMPORTANT**: El que es valorarà en la pràctica és la capacitat de mantenir una narrativa coherent alhora que s'expliquen els resultats. No es mirarà tant que alguna pregunta quedi per respondre sinó que els passos seguits en base als resultats obtinguts siguin coherents."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. EDA (exploratory data analysis) (1 punt)\n",
    "\n",
    "Abans de res cal sempre veure com es la base de dades assignada.\n",
    "\n",
    "**Preguntes:**\n",
    "- Quants atributs té la vostra base de dades?\n",
    "- Quin tipus d'atributs teniu? (Númerics, temporals, categòrics, binaris...)\n",
    "- Com es el target? quantes categories diferents existeixen?\n",
    "- Tenim nans en les dades?\n",
    "- Podeu veure alguna correlació entre X i y?\n",
    "- Estan balancejades les etiquetes (distribució similar entre categories)? Creieu que pot afectar a la classificació la seva distribució?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "bPrimer, importem la llibreria `pandas` i llegim el fitxer de dades `.csv`."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from xml.sax.handler import feature_external_ges\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.scale import LogisticTransform\n",
    "from statsmodels.tools.linalg import stationary_solve\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "df = pd.read_csv(\"./titanic/train.csv\")\n",
    "DATAFRAME_RAW = df\n",
    "EXPLANATORY = \"Survived\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Podem analitzar els atributs de la base de dades fent servir la comanda `describe()`:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.describe(include=\"all\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Observem que la nostra base de dades conté **12 atributs**, dels següents tipus:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.dtypes",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "On `int64` és un nombre enter (numèric), `float64` és un nombre decimal (numèric) i `object` és text (es podria considerar una etiqueta o categòrica).\n",
    "\n",
    "Però veiem que podem assignar altres tipus en funció del significat de cada columna:\n",
    "\n",
    "- `PassengerId`: etiqueta: identifica a cada persona\n",
    "- `Survived`: binària: `1`=si o `0`=no\n",
    "- `Pclass`: categòrica: `1`=primera, `2`=segona i `3`=tercera classe\n",
    "- `Name`: etiqueta: identifica a cada persona (potser no de forma única)\n",
    "- `Sex`: categòrica: `female` o `male`\n",
    "- `Age`: numèrica: edat en anys\n",
    "- `SibSp`: numèrica: nombre de germans o parelles en el Titanic\n",
    "- `Parch`: numèrica: nombre de pares o fills en el Titanic\n",
    "- `Ticket`: text: codi del tiquet\n",
    "- `Fare`: numèrica: preu del tiquet\n",
    "- `Cabin`: text: codi de la cabina\n",
    "- `Embarked`: categòrica: port d'embarcació, `C`=Cherbourg, `Q`=Queenstown i `S`=Southampton\n",
    "\n",
    "Per aquest estudi, el target és l'atribut binari `Survived`, que pot prendre dos valors: 0 o 1:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[\"Survived\"].unique()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Per analitzar la qualitat de les dades, mirem quantes files tenen dades incompletes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.isna().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "De les 891 dades, falta el port d'embarcació `Embarked` per a dues persones, l'edat per a 177 i la cabina per a 687.\n",
    "\n",
    "Les dues persones sense port d'embarcació no son molt importants, podríem fins i tot prescindir d'aquestes files. L'edat podria ser un problema, i la cabina, que d'entrada sembla que pot ser la variable explicativa més important, no apareix en la majoria de files."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Per detectar possibles correlacions en les dades, podem calcular el coeficient de correlació de Pearson entre tots els atributs numèrics i representar-los en una gràfica de calor."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.heatmap(df.corr(numeric_only=True), annot=True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Unes quantes observacions (comentarem la correlació només a una de les dues variables):\n",
    "\n",
    "- `PassengerId`: aquesta no té correlació significant amb cap dels altres atributs (com és d'esperar)\n",
    "- `Survived`: obté la seva màxima correlació (inversament) amb `Pclass` i `Fare`: com més gran la classe, més petita la probabilitat de supervivència, i com més car el tiquet, més probabilitat\n",
    "- `Pclass`: correlació inversa amb el preu del tiquet (està clar) i també amb l'edat, és dir els més joves es trobaven a classes més altes\n",
    "- `Age`: relació inversa amb el nombre de germans/parelles i pares/fills, com més joves més germans/parelles i pares/fills\n",
    "- `SibSp`: correlació amb el nombre de pares/fills, com més fills/parelles, més pares/fills\n",
    "- `Parch`: correlació molt petita amb el preu del tiquet\n",
    "\n",
    "Estudiem ara la distribució de les dades amb histogrames per cada atribut numèric (afegim les columnes `Embarked` i `Sex` al gràfic)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = list(df.select_dtypes(\"number\")) + [\"Sex\", \"Embarked\"]\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, cols):\n",
    "    if df[col].dtype == \"object\":\n",
    "        sns.countplot(x=col, data=df, ax=ax)\n",
    "    else:\n",
    "        df[col].hist(bins=30, ax=ax)\n",
    "    ax.set_title(col)\n",
    "\n",
    "for ax in axes[len(cols):]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Veiem que la distribució de l'edat té forma bastant normal (la distribució), amb una petita acumulació a edats petites. Les dues variables quantitatives enteres (`SibSp` i `Parch`) tenen més representació per a nombres petits.\n",
    "\n",
    "Mirem amb més detall les distribucions de les altres columnes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in [\"Survived\", \"Pclass\", \"Sex\", \"Embarked\"]:\n",
    "    print(f\"{col}:\\n\\t- {\"\\n\\t- \".join([f\"{key}: {value:.2%}\" for key, value in dict(df[col].value_counts(normalize=True)).items()])}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Tenim representació prou bona per totes les etiquetes, només pel port Queenstown una mica menys, però no és res greu.\n",
    "\n",
    "Ara per ara, creiem que la distribució de les dades no influirà molt en la predicció de les dades, i que no donarà problemes. Faltaria fer una selecció, normalització i processament dels atributs per acabar de concloure si la distribució ens afectarà la predicció final, que és el que farem a la següent secció."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Preprocessing (normalitzation, outlier removal, feature selection, ...) (2 punts)\n",
    "Un cop vistes les dades de les que es disposa, cal preparar les dades per als nostres algoritmes. Segons la tipologia de dades, es poden filtrar atributs, aplicar-hi reductors de dimensionalitat, codificar categories textuals en valors numèrics, normalitzar les dades, treure outliers...\n",
    "\n",
    "Navegueu per la [documentació de sklearn sobre preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) per tal de trobar les diferents opcions que proporciona sklearn.\n",
    "\n",
    "**Preguntes:**\n",
    "* Estan les dades normalitzades? Caldria fer-ho?\n",
    "* En cas que les normalitzeu, quin tipus de normalització serà més adient per a les vostres dades?\n",
    "* Teniu gaires dades sense informació (nans)? Tingueu en compte que hi ha metodes que no els toleren durant l'aprenentatge. Com afecta a la classificació si les filtrem? I si les reompliu? Com ho farieu? [Pista](https://scikit-learn.org/stable/modules/impute.html)\n",
    "* Teniu dades categòriques? Quina seria la codificació amb més sentit?\n",
    "* Podreu treure algun atribut extra de les categòriques (per exemple, aplicant alguna regla sobre el text)?\n",
    "* Caldria aplicar PCA? Quins beneficis o inconvenients trobarieu?\n",
    "* Caldria aplicar alguna tècnica de selecció de variables? Ho trobeu necessari?"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Llibreries que farem servir en aquesta secció."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A partir de les conclusions extretes de la secció anterior, comencem a fer el preprocessing de tot el dataset. Ja hem vist que tenim moltes dades que falten, sobretot en el cas de l'edat i la cabina."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nan_count = df.isna().sum()\n",
    "nan_percent = (nan_count / len(df)) * 100\n",
    "nan_info = pd.DataFrame({\n",
    "    \"NaN Count\": nan_count,\n",
    "    \"NaN Percent\": nan_percent\n",
    "})\n",
    "\n",
    "nan_info[nan_info[\"NaN Count\"] > 0]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Començant per la cabina: veiem que falta una quantitat de dades molt gran, quasi el 80%. Per tant, seria molt complicat i gens significatiu aplicar qualsevol tipus d'imputació sobre aquesta columna. També observem que és una dada molt rellevant per la supervivència dels passatgers. Si busquem un esquema del Titanic, podem veure que els decks estan organitzats de la següent manera:"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div>\n",
    "<img src=\"titanic/decks.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Per tant, com podem veure les cabines que son del deck G estan a baix de tot del vaixell, i les de l'A son les de més amunt. Creiem que el deck és una dada molt important per predir la supervivència, per tant, l'extraiem a partir del codi de cabina. Veiem que els codis son el deck més un nombre de cabina, sobre el qual no hem pogut trobar més informació.\n",
    "\n",
    "Mirem els decks que surten a la base de dades."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "sorted(df.loc[df[\"Cabin\"].notna(), \"Cabin\"].str[0].unique())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Tenim cabines de tots els decks, i del `T`. Investiguem quantes cabines hi ha del deck `T`, que no sabem on és.b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[df[\"Cabin\"].str.contains(\"T\", na=False)]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Com només és un passatger, considerem que és un error, per tant, marcarem el deck d'aquest passatger com un NaN. Amb totes les altres cabines que falten, marcarem els decks de totes amb la lletra `X` per distingir-les de les cabines normals.\n",
    "\n",
    "Volem poder generalitzar aquest preprocessing a qualsevol dataset, per tant, creem una funció personalitzada que faci aquesta anàlisi de la cabina i la creï la nova columna `Deck` en forma d'un transformer."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_deck(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Copiar el dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    # Extreure la primera lletra de la cabina a una nova columna\n",
    "    dataframe[\"Deck\"] = dataframe[\"Cabin\"].str[0]\n",
    "\n",
    "    # Validar els decks existents\n",
    "    valid_decks = {\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\"}\n",
    "    dataframe[\"Deck\"] = dataframe[\"Deck\"].where(dataframe[\"Deck\"].isin(valid_decks), \"X\")\n",
    "\n",
    "    # Omplir els forats amb \"X\"\n",
    "    dataframe[\"Deck\"] = dataframe[\"Deck\"].fillna(\"X\")\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Crear un transformer\n",
    "extract_deck_transformer = FunctionTransformer(extract_deck, validate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Podem provar el pipeline fet amb el dataframe que tenim, aplicant-li només aquesta transofrmació."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "extract_deck_transformer.transform(df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Veiem que tenim una columna extra: `Deck`.\n",
    "\n",
    "A part d'això, també considerem que és important saber simplement si tenim informació sobre la cabina, observem les proporcions de cabines que falten en funció de la classe."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.groupby(\"Pclass\")[\"Cabin\"].apply(lambda x: x.isna().mean())",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Clarament, la majoria de cabines que tenim son de la classe 1. Per tant, com és una dada rellevant, afegim aquest indicador."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def indicate_missing_cabin(dataframe: pd.DataFrame, column: str = \"Cabin\") -> pd.DataFrame:\n",
    "    # Copiar el dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    # Crear una columna que indiqui si Cabin és available o no\n",
    "    dataframe[column + \"__missing\"] = dataframe[column].isna().astype(int)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Crear un transformer\n",
    "indicate_missing_cabin_transformer = FunctionTransformer(indicate_missing_cabin, validate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I comprovem que funciona:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "indicate_missing_cabin_transformer.transform(df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Anem a estudiar ara la columna `Embarked`. Sabem que només hi ha dues files, anem a veure-les:b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[df[\"Embarked\"].isna()]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Sabem que aquesta columna indica el port on ha pujat el passatger, per tant, podríem estimar a quin port ha pujat el passatger mirant els altres passatgers que han pagat un preu similar (òviament considerant la mateixa classe).\n",
    "\n",
    "Per tant, mirem els passatgers amb la mateixa classe i preu similar, considerant més o menys 5 dòlars. Calculem la freqüència absoluta de cada port."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[(df[\"Embarked\"].notna()) & (df[\"Pclass\"] == 1) & (75 <= df[\"Fare\"]) & (df[\"Fare\"] <= 85)][\"Embarked\"].value_counts()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Veiem que aquests filtres inclouen 29 passatgers, dels quals 16 has pujat al port `C`, Cherbourg.\n",
    "\n",
    "Sembla raonable considerar aquest mini-algoritme per determinar el port d'embarcació més probable. Implementem-lo amb una funció transformer com abans."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def impute_embarked(dataframe: pd.DataFrame, tolerance: float = 10.0) -> pd.DataFrame:\n",
    "    # Copiar el dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    for idx, row in dataframe[dataframe[\"Embarked\"].isna()].iterrows():\n",
    "        pclass = row[\"Pclass\"]\n",
    "        fare = row[\"Fare\"]\n",
    "\n",
    "        # Candidats: mateix Pclass, Fare amb tolerància de +-10\n",
    "        candidates = df[\n",
    "            (df[\"Pclass\"] == pclass) &\n",
    "            (df[\"Embarked\"].notna()) &\n",
    "            (df[\"Fare\"] >= fare - 10) &\n",
    "            (df[\"Fare\"] <= fare + 10)\n",
    "        ]\n",
    "\n",
    "        # Si s'han trobat candidats\n",
    "        if not candidates.empty:\n",
    "            dataframe.at[idx, \"Embarked\"] = candidates[\"Embarked\"].mode()[0]\n",
    "        else:\n",
    "            # Si no s'han trobat candidats, agafar la moda de tot el dataframe\n",
    "            dataframe.at[idx, \"Embarked\"] = dataframe[\"Embarked\"].mode()[0]\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Crear un transformer\n",
    "impute_embarked_transformer = FunctionTransformer(impute_embarked, validate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Comprovem que funciona, veient que no hi ha cap dada amb el port d'embarcació no disponible."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "impute_embarked_transformer.transform(df)[\"Embarked\"].isna().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Ara hem de tractar les edats que falten. Representen una cinquena part de les dades, per tant, intentarem imputar fent servir algun algoritme o relació que observem amb els altres atributs.\n",
    "\n",
    "Fem una primera gràfica amb la quantitat de passatgers amb una certa edat separant les classes. En aquest cas, no ens interessa el valor absolut de la quantiat de gent, només la proporció entre cada classe, per tant, indiquem l'opció `fill` per obtenir un sentit de les proporcions per cada franja d'edat."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(\n",
    "    data=df,\n",
    "    x=\"Age\",\n",
    "    hue=\"Pclass\",\n",
    "    multiple=\"fill\",\n",
    "    bins=40\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Age distribution by Pclass\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Veiem que la classe 1 està centrada més a la dreta, i les classes 2 i 3 més a l'esquerra. Lo important d'aquesta gràfica és que les tres classes no tenen proporcions constants per totes les edats. Mirem ara les mitjanes d'edats per cada classe."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.groupby(\"Pclass\")[\"Age\"].mean()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Veiem que hi ha una diferència important d'edats entre les classes.\n",
    "\n",
    "A partir d'aquesta informació, ja podríem calcular una mediana de les classes, i imputar aquest valor a cada passatger, però pensem que és massa brut fer-ho per una cinquena part del dataset, introduirà molta uniformitat i canviarà la distribució. Com ja hem vist amb el mapa de calor dels coeficients de correlació, l'edat tenia relació amb el nombre de germans/parelles i fills/pares, per tant, considerem que podem treure més informació de les dades que tenim per aconseguir una predicció més ajustada. Pensant en les noves columnes que hem afegit, creiem que podríem expandir les dades que tenim per aconseguir una predicció millor que només la moda. Mirem, per exemple, la distribució en funció de si es té informació de la cabina o no (per això, hem d'aplicar el transformer corresponent a la base de dades)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(\n",
    "    data=indicate_missing_cabin_transformer.transform(df),\n",
    "    x=\"Age\",\n",
    "    hue=\"Cabin__missing\",\n",
    "    multiple=\"fill\",\n",
    "    bins=40\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Age distribution by Missing Cabin Indicator\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "A més, mirant les freqüències absolutes observem que també hi ha una gran diferència a la quantitat de gent en funció d'aquest indicador. En aquesta gràfica els dos grups (amb, i sense cabina indicada) estan superposats."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sns.histplot(\n",
    "    data=indicate_missing_cabin_transformer.transform(df),\n",
    "    x=\"Age\",\n",
    "    hue=\"Cabin__missing\",\n",
    "    bins=40\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Age distribution by Missing Cabin Indicator\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Mirem totes les columnes que tenim disponibles (considerant les transformacions ja fetes): `PassengerId`, `Survived`, `Pclass`, `Name`, `Sex`, `Age`, `SibSp`, `Parch`, `Ticket`, `Fare`, `Cabin`, `Embarked`, `Deck`,`Cabin__missing`.\n",
    "\n",
    "Fent un cop d'ull a la columna `Name`, veiem que hi ha noms que contenen tractaments, com \"Mr.\", \"Mrs.\" i \"Master.\". Aquest podrien constituir un molt bon indicador de la seva edat, ja que, per exemple, master es fa servir per nens petits. Mirem tots els tractaments que apareixen (hem de fer servir la funció `extract` amb un regex per extreure el tractament abans del punt), i analitzem quants passatgers tenim per cada un amb, i sense edat."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.groupby(df[\"Name\"].str.extract(r\"(\\w+)\\.\", expand=False))[\"Age\"].agg(\n",
    "    WithAge=lambda x: x.notna().sum(),\n",
    "    MissingAge=lambda x: x.isna().sum()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Veiem que hi ha molts títols que son el mateix escrit de dues formes, només apareixen per a una persona o son similars en significat, així que resumirem aquesta llista a unes categories més generals. Això ho farem en la funció transformer.\n",
    "\n",
    "Tornem a l'anàlisi: agafem per exemple el títol \"Master\", i mirem la mitjana d'edat."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[df[\"Name\"].str.contains(\"Master.\", regex=False)][\"Age\"].mean()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Clarament, pels passatgers amb títol master imputarem una edat semblant a aquesta, és dir molt petita.\n",
    "\n",
    "Per poder treballar millor amb aquestes tractaments, creem un transformer que afegeixi aquesta columna a la nostra base de dades (fent el resum que comentàvem abans)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_title(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Copiar el dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    # Extreure el tractament i guardar-lo en una nova columna\n",
    "    dataframe[\"Title\"] = dataframe[\"Name\"].str.extract(r\"(\\w+)\\.\", expand=False)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Crear un transformer\n",
    "extract_title_transformer = FunctionTransformer(extract_title, validate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Veiem que realment funciona."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "extract_title_transformer.transform(df)[\"Title\"].value_counts()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I que no té cap cel·la sense títol."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "extract_title_transformer.transform(df)[\"Title\"].isna().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Pensant intuïtivament, és probable que com més gran sigui la familia de cada individu, més petita serà l'edad mitjana, considerant que la major part son nens. Per tant, proposem una nova columna agregada que, en aquest cas, és simplement la suma del nombre de germans/parelles i pares/fills. Com sempre, creem una funció per dur a terme aquesta transformació."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def add_family_size(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Copiar el dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    # Extreure els membres de la familia (+1 per aquell passatger)\n",
    "    dataframe[\"FamilySize\"] = dataframe[\"SibSp\"] + dataframe[\"Parch\"] + 1\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "# Crear un transformer\n",
    "add_family_size_transformer = FunctionTransformer(add_family_size, validate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Comprovem que funciona correctament."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "add_family_size_transformer.transform(df)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "logistic_regressor_tester.plot_decision_boundary(features=[\"Age\", \"Fare\"])\n",
    "Comprovem ràpidament la nostra hipòtesi mirant només el coeficient de correlació de Pearson (que detecta relació de linealitat entre les dues variables)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df[\"Age\"].corr(add_family_size_transformer.transform(df)[\"FamilySize\"])",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Dona negatiu, com esperàvem, és dir que a més membres en la familia, menor és l'edat. En valor absolut no és gaire convincent, es considera un valor baix, però de moment inclourem aquesta dada per fer la predicció de l'edat.\n",
    "\n",
    "Amb totes les noves features que hem creat en la base de dades, ja podem pensar en com podem predir l'edat per a les persones que no en tenen una indicada. Estem en un cas ideal per fer servir l'algoritme KNN. Té sentit que si dos passatgers estan \"a prop\" en aquest espai multidimensional, tinguin edats similars. Per això, farem servir les variables categòriques títol i deck (que passarem a variables indicadores binàries), les numèriques preu, germans/parelles, pares/fills, membres de la familia i la binària que ens diu si té indicada una cabina.\n",
    "\n",
    "Com aquest algoritme KNN és sensible a l'escala de les variables (ja que tracta amb distàncies), és necessari aplicar una normalització de les dades numèriques abans de començar el procés iteratiu.\n",
    "\n",
    "Implementem tot això a través d'un pipeline que automàticament ens passa les categòriques a binàries, normalitza i prediu les edats (després d'incloure tots els transformers fets anteriorment)."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "categorical_features = [\"Title\", \"Cabin__missing\", \"Pclass\"]\n",
    "numerical_features = [\"Fare\", \"SibSp\", \"Parch\", \"FamilySize\"]\n",
    "\n",
    "age_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), categorical_features),\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "        (\"pred\", \"passthrough\", [\"Age\"])\n",
    "    ]\n",
    ")\n",
    "\n",
    "age_pipeline = Pipeline([\n",
    "    (\"missing_cabin\", indicate_missing_cabin_transformer),\n",
    "    (\"extract_title\", extract_title_transformer),\n",
    "    (\"add_family_size\", add_family_size_transformer),\n",
    "    (\"preprocessor\", age_preprocessor),\n",
    "    (\"imputer\", KNNImputer(n_neighbors=5))\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Executem el pipeline i guardem el resultat final per fer unes observacions."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "new_df = pd.DataFrame(age_pipeline.fit_transform(df), columns=age_pipeline.named_steps[\"preprocessor\"].get_feature_names_out())\n",
    "new_df.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Mirem, primer, que ara sí tinguem totes les dades sense NaN's (entre les columnes que hem fet servir en el preprocessador pel KNN, per exemple cabines segueix tenint NaN's)."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "new_df.isna().sum().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ara comparem les edats mitjanes per títol abans, i després de l'imputament amb el KNN."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_titled = extract_title_transformer.transform(df)\n",
    "df_titled[\"Imputed Age\"] = new_df.iloc[:, -1]\n",
    "age_comparison = pd.DataFrame({\n",
    "    \"Original_Age\": df_titled.groupby(\"Title\")[\"Age\"].mean(),\n",
    "    \"Imputed_Age\": df_titled.groupby(\"Title\")[\"Imputed Age\"].mean(),\n",
    "})\n",
    "age_comparison[\"NaN_Count\"] = df_titled.groupby(\"Title\")[\"Age\"].apply(lambda x: x.isna().sum())\n",
    "age_comparison[\"Difference\"] = age_comparison[\"Imputed_Age\"] - age_comparison[\"Original_Age\"]\n",
    "\n",
    "age_comparison"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Podem observar que l'algoritme KNN ha estat molt fidel amb el que nosaltres esperàvem, que les edats siguin semblants pel mateix títol. Veiem que les mitjanes per honorífic han canviat molt poc.\n",
    "\n",
    "Analitzem també les distribucions de les edats abans i després de l'imputament."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.hist(df_titled[\"Imputed Age\"], bins=30, alpha=0.5, label=\"Imputed Age\")\n",
    "plt.hist(df_titled[\"Age\"].dropna(), bins=30, alpha=0.5, label=\"Original Age\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Histogram of Original vs Imputed Age\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Observem que la distribució de les edats imputades segueix prou precisament la forma de les edats originals, per tant donarem aquests resultats per bons.\n",
    "\n",
    "Amb aquesta última anàlisi, acabem de tractar totes les columnes amb informació mancant. Fem un repàs:\n",
    "\n",
    "- `Cabin`: hem extret la columna `Deck` (`extract_deck_transformer`) i hem afegit un indicador `Cabin__missing` per saber si disposem de la dada (`indicate_missing_cabin_transformer`)\n",
    "- `Embark`: hem extret els passatgers de la mateixa classe i preu semblant i hem agafat la moda (`impute_embarked_transformer`)\n",
    "- `Age`: hem vist que `Pclass` i `Cabin__missing` son importants per predir l'edat, hem extret el títol del nom per crear `Title` (`extract_title_transformer`) i la mida de la família a partir dels germans/parelles i pares/fills per crear `FamilySize` (`add_family_size_transformer`), i amb totes aquestes dades hem fet un KNN per predir les edats que faltaven (`age_pipeline`)\n",
    "\n",
    "Com més endavant voldrem enllaçar tots els transformers en una pipeline, millorem el transformer de predicció de l'edat. Ho farem creant una classe custom a partir de les classes base de sklearn, implementant les funcions base `fit` i `transform`."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class AgeKNNImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_neighbors=5):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.age_features = [\"Fare\", \"SibSp\", \"Parch\", \"FamilySize\", \"Title\", \"Cabin__missing\", \"Pclass\"]\n",
    "        self.ohe = None\n",
    "        self.scaler = None\n",
    "        self.knn = None\n",
    "        self.column_transformer = None\n",
    "        self.feature_names_out_ = None\n",
    "\n",
    "    @staticmethod\n",
    "    def create_necessary_features(dataframe_x: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Crear les features que calen si no hi son\n",
    "        if \"FamilySize\" not in dataframe_x.columns:\n",
    "            dataframe_x = add_family_size_transformer.transform(dataframe_x)\n",
    "        if \"Title\" not in dataframe_x.columns:\n",
    "            dataframe_x = extract_title_transformer.transform(dataframe_x)\n",
    "        if \"Cabin__missing\" not in dataframe_x.columns:\n",
    "            dataframe_x = indicate_missing_cabin_transformer.transform(dataframe_x)\n",
    "\n",
    "        return dataframe_x\n",
    "\n",
    "    def fit(self, dataframe_x, y=None):\n",
    "        # Copiar el dataframe\n",
    "        dataframe_x = dataframe_x.copy()\n",
    "\n",
    "        # Crear les features que no existeixen\n",
    "        dataframe_x = self.create_necessary_features(dataframe_x)\n",
    "\n",
    "        # Features\n",
    "        cat_fts = [\"Title\", \"Cabin__missing\", \"Pclass\"]\n",
    "        num_fts = [\"Fare\", \"SibSp\", \"Parch\", \"FamilySize\"]\n",
    "\n",
    "        # Transformadors\n",
    "        self.column_transformer = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), cat_fts),\n",
    "                (\"num\", StandardScaler(), num_fts),\n",
    "                (\"pred\", \"passthrough\", [\"Age\"])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Fit dels transformadors\n",
    "        self.column_transformer.fit(dataframe_x)\n",
    "\n",
    "        # Fit KNNImputer\n",
    "        transformed = self.column_transformer.transform(dataframe_x)\n",
    "        self.knn = KNNImputer(n_neighbors=self.n_neighbors)\n",
    "        self.knn.fit(transformed)\n",
    "\n",
    "        # Guardar els noms de les columnes\n",
    "        cat_features_out = self.column_transformer.named_transformers_[\"cat\"].get_feature_names_out(categorical_features)\n",
    "        self.feature_names_out_ = np.concatenate([cat_features_out, numerical_features, [\"Age\"]])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, dataframe_x):\n",
    "        # Copiar el datagram\n",
    "        dataframe_x = dataframe_x.copy()\n",
    "        \n",
    "        # Crear les features que no existeixen\n",
    "        dataframe_x = self.create_necessary_features(dataframe_x)\n",
    "\n",
    "        # Predir l'edat\n",
    "        transformed = self.column_transformer.transform(dataframe_x)\n",
    "        imputed = self.knn.transform(transformed)\n",
    "\n",
    "        # Copiar els resultats\n",
    "        result = dataframe_x.copy()\n",
    "        result[\"Age\"] = imputed[:, -1]\n",
    "\n",
    "        return result"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Provem si funciona la classe igual que abans."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "age_knn_processor = AgeKNNImputer(n_neighbors=5)\n",
    "age_knn_processor.fit(df)\n",
    "new_df = age_knn_processor.transform(df)\n",
    "new_df.isna().sum()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Veiem que retorna les mateixes columnes (els mateixos noms) més totes les auxiliars que es necessiten per fer la predicció. També observem que efectivament no queden valors nul·ls a la columna de les edats.\n",
    "\n",
    "**IMPORTANT**: encara no hem encadenat tots els transformadors, és normal que tinguem embarked i cabines NaN's, en cap moment li hem aplicat el transformador corresponent per treure-les, al final de la secció farem un pipeline general amb totes les funcions.\n",
    "\n",
    "En aquest moment ja tenim transformadors per aprofitar al màxim la informació de totes les columnes.\n",
    "\n",
    "De la columna `Title` recordem que tenim moltes categories, de les quals la majoria son per a només poques persones. Com després haurem de passar aquesta variable a indicadors binaris, no volem tenir la meitat de features que siguin indicadors del títol, per tant, reduïm les categories d'aquesta variable a només unes quantes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compress_title(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Copiar el dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    # Diccionari de categories\n",
    "    mapping = {\n",
    "        \"Capt\": \"Military\", \"Col\": \"Military\", \"Major\": \"Military\",\n",
    "        \"Don\": \"Nobility\", \"Dona\": \"Nobility\", \"Jonkheer\": \"Nobility\",\n",
    "        \"Lady\": \"Nobility\", \"Sir\": \"Nobility\", \"Countess\": \"Nobility\",\n",
    "        \"Master\": \"Nobility\", \"Miss\": \"Ms\", \"Mlle\": \"Ms\", \"Ms\": \"Ms\",\n",
    "        \"Mrs\": \"Mrs\", \"Mme\": \"Mrs\"\n",
    "    }\n",
    "\n",
    "    # Fer el mapeig\n",
    "    dataframe[\"Title\"] = dataframe[\"Title\"].map(mapping).fillna(dataframe[\"Title\"])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "compress_title_transformer = FunctionTransformer(compress_title, validate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Veiem que funciona."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "compress_title_transformer.transform(extract_title_transformer.transform(df))[\"Title\"].value_counts()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "De la columna del tiquet no hem pogut trobar ni extreure cap informació valuosa per la predicció, així que decidim esborrar-la.\n",
    "\n",
    "Tornem ara a les distribucions de les dades numèriques i categòriques per decidir si tenim outliers o si caldria limitar el domini de cada atribut. Ho fem només ara perquè ara sí que tenim totes les dades que necessitem (sobretot, les edats), i també hem de considerar la columna creada `FamilySize`. La columna `Pclass` no la tindrem en compte perquè ja hem vist que està equilibrada i no hi ha valors extrems.\n",
    "\n",
    "Creem un pipeline ràpid per afegir les columnes que falten i imputar les edats."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "temp_preprocessor = Pipeline(steps=[\n",
    "    (\"extract_deck\", extract_deck_transformer),\n",
    "    (\"missing_cabin\", indicate_missing_cabin_transformer),\n",
    "    (\"impute_embarked\", impute_embarked_transformer),\n",
    "    (\"extract_title\", extract_title_transformer),\n",
    "    (\"add_family_size\", add_family_size_transformer),\n",
    "    (\"impute_age\", AgeKNNImputer()),\n",
    "    (\"compress_title\", compress_title_transformer),\n",
    "])\n",
    "\n",
    "temp_df = temp_preprocessor.fit_transform(df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\", \"Deck\", \"Embarked\", \"Title\", \"Cabin__missing\", \"Pclass\"]\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, cols):\n",
    "    if temp_df[col].dtype == \"object\":\n",
    "        sns.countplot(x=col, data=temp_df, ax=ax)\n",
    "    else:\n",
    "        temp_df[col].hist(bins=30, ax=ax)\n",
    "    ax.set_title(col)\n",
    "\n",
    "for ax in axes[len(cols):]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "La idea és trobar els valors \"outlier\", o atípics. Podem aconseguir això mirant l'interval interquartílic. Creem un transformador que ens retalli les variables numèriques a 1.5 cops aquest interval i veiem els resultats."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class IQRTrimmer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, factor=1.5):\n",
    "        self.factor = factor\n",
    "        self.bounds_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = pd.DataFrame(X)\n",
    "        for col in X.select_dtypes(include=[np.number]).columns:\n",
    "            q1 = X[col].quantile(0.25)\n",
    "            q3 = X[col].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower = q1 - self.factor * iqr\n",
    "            upper = q3 + self.factor * iqr\n",
    "            self.bounds_[col] = (lower, upper)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.DataFrame(X).copy()\n",
    "        for col, (lower, upper) in self.bounds_.items():\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].clip(lower=lower, upper=upper)\n",
    "        return X\n",
    "\n",
    "trimmer = IQRTrimmer()\n",
    "temp_df_trimmed = trimmer.fit_transform(temp_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Repetim les gràfiques amb el nou dataframe."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\", \"Deck\", \"Embarked\", \"Title\", \"Cabin__missing\", \"Pclass\"]\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, cols):\n",
    "    if temp_df_trimmed[col].dtype == \"object\":\n",
    "        sns.countplot(x=col, data=temp_df_trimmed, ax=ax)\n",
    "    else:\n",
    "        temp_df_trimmed[col].hist(bins=30, ax=ax)\n",
    "    ax.set_title(col)\n",
    "\n",
    "for ax in axes[len(cols):]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Veiem que els resultats no son els esperats. De la columna `Cabin__missing` no volem perdre tota la informació que tenim (que seria posar que tots tenen cabin), per tant, per cada columna farem un tractament diferent:\n",
    "\n",
    "- `Age`: agafarem un threshold aproximat de 65\n",
    "- `Parch` i `SibSp`: limitarem a 0, 1 o 2 (considerant 2 com 2 o més)\n",
    "- `FamilySize`: limitarem a 1, 2, 3 i 4 (considerant 4 com 4 o més)\n",
    "- `Fare`: limitarem al preu aproximate de 100\n",
    "\n",
    "Deixarem les altres variables com estan."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def custom_threshold(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Copiar el dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    # Age: threshold a 65\n",
    "    if \"Age\" in dataframe.columns:\n",
    "        dataframe[\"Age\"] = np.where(dataframe[\"Age\"] > 65, 65, dataframe[\"Age\"])\n",
    "\n",
    "    # Parch i SibSp: màxim 2\n",
    "    for col in [\"Parch\", \"SibSp\"]:\n",
    "        if col in dataframe.columns:\n",
    "            dataframe[col] = np.where(dataframe[col] > 2, 2, dataframe[col])\n",
    "\n",
    "    # FamilySize: màxim 4\n",
    "    if \"FamilySize\" in dataframe.columns:\n",
    "        dataframe[\"FamilySize\"] = np.where(dataframe[\"FamilySize\"] > 4, 4, dataframe[\"FamilySize\"])\n",
    "\n",
    "    # Fare: threshold a 100\n",
    "    if \"Fare\" in dataframe.columns:\n",
    "        dataframe[\"Fare\"] = np.where(dataframe[\"Fare\"] > 100, 100, dataframe[\"Fare\"])\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "custom_threshold_transformer = FunctionTransformer(custom_threshold, validate=False)\n",
    "temp_df_threshold = custom_threshold_transformer.transform(temp_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Analitzem les noves distribucions."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\"]\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, cols):\n",
    "    if temp_df_threshold[col].dtype == \"object\":\n",
    "        sns.countplot(x=col, data=temp_df_threshold, ax=ax)\n",
    "    else:\n",
    "        temp_df_threshold[col].hist(bins=30, ax=ax)\n",
    "    ax.set_title(col)\n",
    "\n",
    "for ax in axes[len(cols):]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Considerem que les noves distribucions tenen sentit, així que les deixarem per la versió final del dataframe.\n",
    "\n",
    "Els models d'aprenentatge que farem servir només entenen atributs numèrics, així que ens hem de desfer dels que no ho son. Els categòrics (`Sex`, `Embarked`, `Title`, `Deck`) els podrem passar a binaris (també s'accepten com a 0 o 1), així que aquests els deixem. De la columna `Name` ja no en traurem més profit a part del `Title`, així que decidim esborrar aquesta columna del dataset final. Passa igual amb la cabina, ja li hem extret el `Deck` i no podem fer servir cap més informació a part d'aquesta, la decidim esborrar també."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def normalize_numerics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    num_cols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\", \"Pclass\"]\n",
    "    df_num = df[num_cols].copy()\n",
    "    for col in num_cols:\n",
    "        mean = df_num[col].mean()\n",
    "        std = df_num[col].std()\n",
    "        df_num[col] = (df_num[col] - mean) / std if std != 0 else 0\n",
    "    other_cols = df.drop(columns=num_cols)\n",
    "    df_final = pd.concat([df_num, other_cols], axis=1)\n",
    "    return df_final\n",
    "\n",
    "normalize_numerics_transformer = FunctionTransformer(normalize_numerics, validate=False)\n",
    "normalized_df = normalize_numerics_transformer.fit_transform(temp_df_threshold)\n",
    "\n",
    "def encode_categoricals(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    cat_cols = [\"Sex\", \"Embarked\", \"Deck\", \"Title\"]\n",
    "    df_encoded = pd.get_dummies(df[cat_cols], drop_first=True, dummy_na=False)\n",
    "    other_cols = df.drop(columns=cat_cols)\n",
    "    df_final = pd.concat([df_encoded, other_cols], axis=1)\n",
    "    return df_final\n",
    "\n",
    "encode_categoricals_transformer = FunctionTransformer(encode_categoricals, validate=False)\n",
    "encoded_df = encode_categoricals_transformer.fit_transform(normalized_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Podem observar les dades normalitzades."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"FamilySize\", \"Pclass\"]\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, col in zip(axes, cols):\n",
    "    if normalized_df[col].dtype == \"object\":\n",
    "        sns.countplot(x=col, data=normalized_df, ax=ax)\n",
    "    else:\n",
    "        normalized_df[col].hist(bins=30, ax=ax)\n",
    "    ax.set_title(col)\n",
    "\n",
    "for ax in axes[len(cols):]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Fem un transformer ràpid per eliminar les columnes."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def drop_column(dataframe, columns):\n",
    "    # Copiar el dataframe\n",
    "    dataframe = dataframe.copy()\n",
    "\n",
    "    # Esborrar les columnes\n",
    "    return dataframe.drop(columns=columns, errors=\"ignore\")\n",
    "\n",
    "drop_cols = {\"columns\": [\"Name\", \"Cabin\", \"Ticket\"]}\n",
    "\n",
    "drop_transformer = FunctionTransformer(drop_column, kw_args=drop_cols, validate=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finalment, construïm el pipeline final amb totes les implementacions que hem fet."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor = Pipeline(steps=[\n",
    "    (\"extract_deck\", extract_deck_transformer),\n",
    "    (\"missing_cabin\", indicate_missing_cabin_transformer),\n",
    "    (\"impute_embarked\", impute_embarked_transformer),\n",
    "    (\"extract_title\", extract_title_transformer),\n",
    "    (\"add_family_size\", add_family_size_transformer),\n",
    "    (\"impute_age\", AgeKNNImputer()),\n",
    "    (\"compress_title\", compress_title_transformer),\n",
    "    (\"custom_threshold\", custom_threshold_transformer),\n",
    "    (\"normalize\", normalize_numerics_transformer),\n",
    "    (\"encode\", encode_categoricals_transformer),\n",
    "    (\"drop\", drop_transformer),\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Apliquem aquest pipeline al dataframe per comprovar que funciona correctament."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "processed_df = preprocessor.fit_transform(df)\n",
    "DATAFRAME = processed_df\n",
    "display(processed_df)\n",
    "display(processed_df.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Per raons que es veuran a validació creuada, també necessitem el dataframe sense normalitzar, per tant, copiem el pipeline sense l'estandarització:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "preprocessor_unnormalized = Pipeline(steps=[\n",
    "    (\"extract_deck\", extract_deck_transformer),\n",
    "    (\"missing_cabin\", indicate_missing_cabin_transformer),\n",
    "    (\"impute_embarked\", impute_embarked_transformer),\n",
    "    (\"extract_title\", extract_title_transformer),\n",
    "    (\"add_family_size\", add_family_size_transformer),\n",
    "    (\"impute_age\", AgeKNNImputer()),\n",
    "    (\"compress_title\", compress_title_transformer),\n",
    "    (\"custom_threshold\", custom_threshold_transformer),\n",
    "    (\"encode\", encode_categoricals_transformer),\n",
    "    (\"drop\", drop_transformer),\n",
    "])\n",
    "\n",
    "processed_df_unnormalized = preprocessor_unnormalized.fit_transform(df)\n",
    "DATAFRAME_UNNORMALIZED = processed_df_unnormalized"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I no obtenim cap cel·la sense valors."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "processed_df.isna().sum().sum()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Per acabar aquesta secció comentarem dues coses importants: PCA i normalització.\n",
    "\n",
    "La primera: Primary Component Analysis. La utilitat principal d'aquest mètode és reduir la dimensionalitat d'un problema si aquest és molt gran o si pensem que hi ha dimensions que no aporten informació a la predicció. En el nostre cas, no es dona cap de les situacions. La dimensió és relativament petita, i a priori pensem que totes les variables son importants. A més, tenim una explicació molt fàcil i interpretació directa a partir de les quantitats. Aplicar PCA es carregaria aquesta interpretabilitat, potser augmentant lleugerament el temps de còmput (un cop entrenat), però perdrem un gran part del problema que és la interpretació de les dades, bàsicament el treball que hem estat fent en aquestes dues seccions.\n",
    "\n",
    "La segona: Normalització. Quan estem treballant amb algoritmes que fan servir distàncies, és normal aplicar una normalització a tots els atributs per així no donar prioritat a les magnituds que potser simplement estan en una altra escala. Per evitar això, normalment s'agafen totes les dades i se les aplica una transformació lineal per centrar-les al zero i amb variació 1. Aquesta és la Standard, la més normal. També hi ha la MinMax, que porta tot el rang de valor a un altre interval desitjat. Com estem treballant amb pipelines i ja tenim funcions implementades per fer aquestes normalitzacions, és molt fàcil afegir o treure-les quan vulguem. Sabem que per alguns mètodes està bé conservar la magnitud (en general és igual), així que, de moment, deixarem les dades amb les magnituds reals i després, per cada model, afegirem una línia que aplicarà la normalització abans."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Metric selection (1.5 punts)\n",
    "En aquest apartat ens centrarem en les mètriques de classificació ([documentació](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)). Per a fer-ho, entreneu una regressio logistica i a partir d'aquesta generarem una serie de funcions per analitzar els nostres resultats. Aquestes funcions ens serviran més endavant. Caldrà tambe triar la mètrica que farem servir després per triar el millor model.\n",
    "\n",
    "**Preguntes:**\n",
    "* A teoria, hem vist el resultat d'aplicar el `accuracy_score` sobre dades no balancejades. Podrieu explicar i justificar quina de les següents mètriques será la més adient pel vostre problema? `accuracy_score`, `f1_score` o `average_precision_score`?\n",
    "* Abans de començar a entrenar models, genereu una suite de funcions per poder analitzar graficament com esta anant el vostre model. Mostreu la Precisió-Recall Curve i la ROC Curve. Quina és més rellevant pel vostre dataset?\n",
    "* Què mostra [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)? Quina mètrica us fixareu per tal de optimitzar-ne la classificació pel vostre cas?\n",
    "\n",
    "**Nota**: Fixeu-vos que en aquest apartat NO ES VALOREN ELS RESULTATS. L'únic que es valora és l'elecció de la mètrica de classificació així com saber quin tipus de gràfiques fer per analitzar els resultats. Abans de solucionar un problema cal tenir molt clar la mètrica d'error que es farà servir, i és una decisió que cal pendre de forma prèvia a entrenar models."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Accuracy: Percentatge de prediccions correctes sobre el total.\n",
    "- Precision: Proporció de prediccions positives que són realment positives (TP / (TP + FP)).\n",
    "- Recall (True Positive Rate, TPR): Proporció de positius reals que el model detecta respecte a la realitat de positius (TP / (TP + FN)).\n",
    "- F1-score: Mitjana harmònica entre precision i recall (equilibri).\n",
    "- Precision-Recall Curve: Gràfic que mostra com canvien precision i recall segons el llindar.\n",
    "    - Eixos: X = Recall, Y = Precision\n",
    "    - Mostra trade-off entre detectar positius i evitar falsos positius.\n",
    "    - Cada punt = un llindar diferent.\n",
    "    - Molt útil amb classes desequilibrades.\n",
    "    - Àrea sota corba = Average Precision (AP)\n",
    "- ROC Curve: Gràfic de True Positive Rate (Recall) vs False Positive Rate a diferents llindars, per cada llindar, es calcula TPR i FPR.\n",
    "    - Eixos: X = FPR (False Positive Rate), Y = TPR (Recall)\n",
    "    - Mostra com canvia TPR respecte a FPR segons el llindar.\n",
    "    - Cada punt = un llindar diferent.\n",
    "    - Útil amb classes equilibrades.\n",
    "    - Àrea sota corba = ROC-AUC (mesura la capacitat de separar positives i negatives).\n",
    "- Threshold en la gràfica: Valor de decisió per classificar una mostra com positiva si la probabilitat estimada és superior o igual a aquest llindar, així havent-hi a la gràfica tants punts com thresholds. Per tant, si tinguéssim una mica menys de probabilitat, per aquell threshold, la mostra es classificaria com negativa. I fixat si fixéssim el threshold una mica menor, obtindríem un punt diferent de la gràfica associat a aquest nou threshold."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.experimental import enable_halving_search_cv  # needed for HalvingGridSearchCV \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, HalvingGridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, roc_curve,\n",
    "    precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, ConfusionMatrixDisplay,\n",
    "    classification_report\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = processed_df[[col for col in processed_df.columns if col != \"Survived\"]]\n",
    "y = processed_df[\"Survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediccions\n",
    "y_pred = lr_model.predict(X_test)\n",
    "y_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "avg_precision = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"\\nResultats amb el model base:\")\n",
    "print(f\"   • Accuracy Score:          {accuracy:.4f}\")\n",
    "print(f\"   • F1 Score:                {f1:.4f}\")\n",
    "print(f\"   • Average Precision Score: {avg_precision:.4f}\")\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "El classification_report mostra diverses mètriques per cada classe i globalment:\n",
    "- Precision: de totes les prediccions positives, quantes són correctes.\n",
    "  Exemple: Sobreviu precision = 0.77 → del total de prediccions “Sobreviu”, el 77% són correctes.\n",
    "- Recall (sensibilitat): de tots els positius reals, quants detectem.\n",
    "  Exemple: Sobreviu recall = 0.74 → detectem el 74% dels supervivents reals.\n",
    "- F1-score: mitjana harmònica de precision i recall, equilibra detectar positius sense generar massa falsos positius.\n",
    "  Exemple: Sobreviu F1 = 0.76 → resumeix el rendiment sobre aquesta classe minoritària.\n",
    "- Support: nombre de mostres reals d’aquesta classe. Exemple: 69 supervivents.\n",
    "- Accuracy: proporció total de prediccions correctes (0.82).\n",
    "- Macro avg: mitjana simple de les mètriques per classe.\n",
    "- Weighted avg: mitjana ponderada pel nombre de mostres de cada classe.\n",
    "\n",
    "Amb el model entrenat, podem avaluar les mètriques en qüestió, però no podem escollir quines mètriques són més adequades sense conèixer la distribució de les classes en el dataset, i què es vol prioritzar."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class_distribution = y.value_counts(normalize=True)\n",
    "majority_class = class_distribution.max()\n",
    "minority_class = class_distribution.min()\n",
    "imbalance_ratio = majority_class / minority_class\n",
    "\n",
    "print(f\"Distribució de classes:\")\n",
    "print(f\"   Classe 0 (No sobreviu): {class_distribution[0]:.2%}\")\n",
    "print(f\"   Classe 1 (Sobreviu):    {class_distribution[1]:.2%}\")\n",
    "print(f\"\\nRàtio de desequilibri: {imbalance_ratio:.2f}:1\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Amb un ràtio de 1.61:1, el dataset NO està severament desequilibrat, però SÍ té un biaix cap a la classe majoritària (No sobreviu, 0). Això afectarà la selecció de mètriques de la següent manera:\n",
    "\n",
    "1. Accuracy: Pot ser enganyosa en datasets desequilibrats.\n",
    "   Al Titanic, encara que no és extremadament desequilibrat, hi ha més morts que supervivents.\n",
    "   Així que un model que prediu gairebé sempre \"mor\" tindria bona accuracy, però seria inútil per detectar els supervivents. Per això no l'hem triada.\n",
    "\n",
    "2. Precision i Recall: Són més informatives per avaluar el rendiment en la classe minoritària (\"Sobreviu\", 1).\n",
    "   - Precision mira quants dels supervivents predits realment sobreviuen (minimitzar falsos positius).\n",
    "   - Recall mira quants dels supervivents reals hem detectat (minimitzar falsos negatius).\n",
    "   Per comparar models cal un equilibri entre ambdues, així que tampoc les hem triades soles.\n",
    "\n",
    "3. Average Precision Score: Resumeix tota la Precision-Recall curve.\n",
    "   És més útil quan el model retorna probabilitats i hi ha molts casos positius escassos.\n",
    "   En aquest cas, com les classes no són extremadament desequilibrades i volem un valor senzill per comparar models, no l’hem triada.\n",
    "\n",
    "4. ROC-AUC ((X,Y)=(FPR,TPR)): També resumeix la capacitat del model per separar classes.\n",
    "   Però és menys informativa que la Precision-Recall curve en casos desequilibrats, ja que pot donar una falsa sensació de bon rendiment quan hi ha molts negatius, ja que el FPR = FP / FP + TN pot ser baix simplement perquè hi ha molts negatius i per tant, TN.\n",
    "\n",
    "5. F1-score: Combina Precision i Recall en una mitjana harmònica, equilibrant la capacitat de detectar supervivents (recall) i no predir falsament supervivents (precision). Per això, hem triat l'F1-score com a mètrica principal per avaluar els models.\n",
    "\n",
    "Sabent això, generem la suite de funcions per visualització a continuació."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Primer, generem una classe *wrapper* per tots els models d'aprenentatge. D'aquesta manera, podem afegir funcionalitat que ens interessa per aquesta pràctica."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Model:\n",
    "    def __init__(self,\n",
    "                 model_class = None,\n",
    "                 dataframe: pd.DataFrame = None,\n",
    "                 hyperparameters: dict = None,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "\n",
    "        self.model_class     = model_class\n",
    "        self.dataframe       = dataframe\n",
    "        self.hyperparameters = hyperparameters or {}\n",
    "\n",
    "        self.explanatory     = kwargs.get(\"explanatory\", \"Survived\")\n",
    "        self.test_size       = kwargs.get(\"test_size\", 0.2)\n",
    "        self.random_state    = kwargs.get(\"random_state\", 1)\n",
    "        self.name            = kwargs.get(\"name\", model_class.__name__)\n",
    "\n",
    "        self.X = dataframe.drop(self.explanatory, axis=1)\n",
    "        self.y = dataframe[self.explanatory]\n",
    "\n",
    "        self.update_dataset(\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "\n",
    "        self.pca = None\n",
    "\n",
    "        self.model = None\n",
    "        self.y_pred = None\n",
    "        self.y_pred_probs = None\n",
    "        self.metrics = {}\n",
    "        self.valid_pred = False\n",
    "\n",
    "        self.search_results_ = None\n",
    "\n",
    "    def update_dataset(self, test_size: float = None, random_state: int = None, features: list = None, pca: int = -1):\n",
    "        if test_size is not None:\n",
    "            self.test_size = test_size\n",
    "        if random_state is not None:\n",
    "            self.random_state = random_state\n",
    "\n",
    "        self.pca = None\n",
    "\n",
    "        self.X = self.dataframe.drop(self.explanatory, axis=1)\n",
    "        self.y = self.dataframe[self.explanatory]\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=self.test_size, random_state=self.random_state)\n",
    "\n",
    "        if features is not None:\n",
    "            # Reduir el dataframe a les columnes indicades\n",
    "            self.pca = None\n",
    "            self.X = self.X[features]\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=self.test_size, random_state=self.random_state)\n",
    "        elif pca != -1:\n",
    "            # Aplicar PCA al training i després convertir el testing a aquelles dimensions\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=self.test_size, random_state=self.random_state)\n",
    "            self.pca = PCA(n_components=pca)\n",
    "            self.pca.fit(self.X_train)\n",
    "            self.X_train = pd.DataFrame(self.pca.transform(self.X_train), columns=[f\"PC{i+1}\" for i in range(pca)])\n",
    "            self.X_test = pd.DataFrame(self.pca.transform(self.X_test), columns=[f\"PC{i+1}\" for i in range(pca)])\n",
    "\n",
    "        self.fit()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit(self):\n",
    "        self.model = self.model_class(**self.hyperparameters)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "\n",
    "        self.valid_pred = False\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, threshold=0.5):\n",
    "        if self.model is None:\n",
    "            self.fit()\n",
    "\n",
    "        if hasattr(self.model, \"predict_proba\"):\n",
    "            self.y_pred_probs = self.model.predict_proba(self.X_test)[:,1]\n",
    "            self.y_pred = (self.y_pred_probs >= threshold).astype(int)\n",
    "        else:\n",
    "            self.y_pred = self.model.predict(self.X_test)\n",
    "            self.y_pred_probs = self.y_pred\n",
    "\n",
    "        self.valid_pred = True\n",
    "\n",
    "        return self.y_pred"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Amb l'estructura d'aquesta classe, ja podem implementar les mètriques que volem amb classes *Helper*.\n",
    "\n",
    "Per les mètriques:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ACCURACY = \"Accuracy\"\n",
    "PRECISION = \"Precision\"\n",
    "RECALL = \"Recall\"\n",
    "F1 = \"F1\"\n",
    "AUC_ROC = \"AUC-ROC\"\n",
    "AUC_PR = \"AUC_PR\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Metrics:\n",
    "    @staticmethod\n",
    "    def evaluate(model: Model):\n",
    "        if not model.valid_pred:\n",
    "            model.predict()\n",
    "        model.metrics = {\n",
    "            ACCURACY: accuracy_score(model.y_test, model.y_pred),\n",
    "            PRECISION: precision_score(model.y_test, model.y_pred),\n",
    "            RECALL: recall_score(model.y_test, model.y_pred),\n",
    "            F1: f1_score(model.y_test, model.y_pred),\n",
    "            AUC_ROC: roc_auc_score(model.y_test, model.y_pred_probs),\n",
    "            AUC_PR: average_precision_score(model.y_test, model.y_pred_probs),\n",
    "        }\n",
    "        return model.metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def confusion_matrix(model: Model):\n",
    "        return confusion_matrix(model.y_test, model.y_pred)\n",
    "\n",
    "    @staticmethod\n",
    "    def summary(model: Model):\n",
    "        if not model.metrics:\n",
    "            Metrics.evaluate(model)\n",
    "        for k, v in model.metrics.items():\n",
    "            print(f\"{k:12s}: {v:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I per les gràfiques:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Plotter:\n",
    "    @staticmethod\n",
    "    def plot_roc(model: Model, plot_ax=None):\n",
    "        if not model.valid_pred:\n",
    "            model.predict()\n",
    "        if not model.metrics:\n",
    "            Metrics.evaluate(model)\n",
    "        if plot_ax is None:\n",
    "            _, plot_ax = plt.subplots()\n",
    "\n",
    "        auc = model.metrics.get(AUC_ROC, None)\n",
    "        title = f\"{model.name}: ROC Curve (AUC: {auc:.4f})\"\n",
    "\n",
    "        fpr, tpr, _ = roc_curve(model.y_test, model.y_pred_probs)\n",
    "\n",
    "        plot_ax.plot(fpr, tpr, color=\"darkorange\", lw=2)\n",
    "        plot_ax.plot([0, 1], [0, 1], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "        plot_ax.set_xlabel(\"False Positive Rate\")\n",
    "        plot_ax.set_ylabel(\"True Positive Rate\")\n",
    "        plot_ax.set_title(title)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_pr(model: Model, plot_ax=None):\n",
    "        if not model.valid_pred:\n",
    "            model.predict()\n",
    "        if not model.metrics:\n",
    "            Metrics.evaluate(model)\n",
    "        if plot_ax is None:\n",
    "            _, plot_ax = plt.subplots()\n",
    "\n",
    "        auc = model.metrics.get(AUC_PR, None)\n",
    "        title = f\"{model.name}: Precision-Recall Curve (AUC: {auc:.4f})\"\n",
    "\n",
    "        prec, recall, _ = precision_recall_curve(model.y_test, model.y_pred_probs)\n",
    "        base = np.mean(model.y_test)\n",
    "\n",
    "        plot_ax.plot(recall, prec, color=\"darkorange\", lw=2)\n",
    "        plot_ax.plot([0, 1], [base, base], color=\"navy\", lw=2, linestyle=\"--\")\n",
    "        plot_ax.set_xlabel(\"Recall\")\n",
    "        plot_ax.set_ylabel(\"Precision\")\n",
    "        plot_ax.set_title(title)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_confusion_matrix(model: Model, plot_ax=None):\n",
    "        if not model.valid_pred:\n",
    "            model.predict()\n",
    "        if plot_ax is None:\n",
    "            _, plot_ax = plt.subplots()\n",
    "\n",
    "        title = f\"{model.name}: Confusion Matrix\"\n",
    "        cm = confusion_matrix(model.y_test, model.y_pred)\n",
    "\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "        disp.plot(ax=plot_ax, cmap=\"Blues\", values_format=\"d\")\n",
    "        plot_ax.set_title(title)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_decision_boundary(model: Model, plot_ax=None, features: list = None, pca: bool = False):\n",
    "        if not model.valid_pred:\n",
    "            model.predict()\n",
    "\n",
    "        X = model.X_test.values\n",
    "        y = model.y_test\n",
    "        feature_names = model.X_test.columns\n",
    "\n",
    "        if features is not None and len(features) == 2:\n",
    "            i, j = feature_names.get_loc(features[0]), feature_names.get_loc(features[1])\n",
    "            X_vis = X[:, [i, j]]\n",
    "            xlabel, ylabel = feature_names[i], feature_names[j]\n",
    "        elif pca:\n",
    "            from sklearn.decomposition import PCA\n",
    "            pca_model = PCA(n_components=2)\n",
    "            X_vis = pca_model.fit_transform(X)\n",
    "            xlabel, ylabel = \"PC1\", \"PC2\"\n",
    "            i, j = None, None\n",
    "        else:\n",
    "            if X.shape[1] > 2:\n",
    "                print(f\"Data has {X.shape[1]} dimensions. Indicate 'features=[...]' or 'pca=2' to reduce to 2D visualization.\")\n",
    "                return None\n",
    "            X_vis = X\n",
    "            xlabel, ylabel = feature_names[0], feature_names[1]\n",
    "            i, j = 0, 1\n",
    "\n",
    "        if plot_ax is None:\n",
    "            _, plot_ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "        f1 = Metrics.evaluate(model)[F1]\n",
    "        title = f\"{model.name}: Decision Boundary (F1: {f1:.4f})\"\n",
    "\n",
    "        h = 0.02\n",
    "        x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
    "        y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "        X_mean = model.X_test.mean().values\n",
    "        X_grid = np.tile(X_mean, (xx.size, 1))\n",
    "\n",
    "        if pca:\n",
    "            grid_proj = pca_model.inverse_transform(np.c_[xx.ravel(), yy.ravel()])\n",
    "            Z = model.model.predict(grid_proj)\n",
    "        else:\n",
    "            X_grid[:, i] = xx.ravel()\n",
    "            X_grid[:, j] = yy.ravel()\n",
    "            Z = model.model.predict(X_grid)\n",
    "\n",
    "        Z = Z.reshape(xx.shape)\n",
    "\n",
    "        plot_ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)\n",
    "        plot_ax.scatter(X_vis[y == 0, 0], X_vis[y == 0, 1], c=\"blue\", edgecolors=\"k\", label=\"Didn't Survive\")\n",
    "        plot_ax.scatter(X_vis[y == 1, 0], X_vis[y == 1, 1], c=\"red\", edgecolors=\"k\", label=\"Survived\")\n",
    "\n",
    "        plot_ax.set_xlabel(xlabel)\n",
    "        plot_ax.set_ylabel(ylabel)\n",
    "        plot_ax.set_title(title)\n",
    "        plot_ax.legend()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Amb aquest codi podem generar les gràfiques i mètriques per avaluar el model entrenat. Ara bé, per què ens interessen aquestes gràfiques si ja tenim la mètrica que ens interessa?\n",
    "\n",
    "La mètrica F1 depèn del llindar de classificació i el que normalment fem és provar diversos llindars per trobar aquell que maximitza el F1; per tant, el F1 “màxim” és el millor que podem aconseguir canviant el llindar. En canvi, les mètriques globals com l’AUC-ROC i l’Average Precision (AP) no depenen d’un llindar concret, sinó que mesuren la qualitat del model a través de tots els llindars possibles, oferint una visió completa del rendiment. Així ajuda a entendre com de bo és el model globalment i a triar un llindar òptim, evitant conclusions enganyoses que podrien sorgir si només et fixes en un llindar fix."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "log_model = Model(\n",
    "    model_class=LogisticRegression,\n",
    "    dataframe=DATAFRAME,\n",
    "    hyperparameters={\"max_iter\":1000}\n",
    ")\n",
    "\n",
    "Metrics.summary(log_model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(24, 10))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "Plotter.plot_roc(log_model, plot_ax=next(axes_iter))\n",
    "Plotter.plot_pr(log_model, plot_ax=next(axes_iter))\n",
    "Plotter.plot_confusion_matrix(log_model, plot_ax=next(axes_iter))\n",
    "Plotter.plot_decision_boundary(log_model, features=[\"Sex_male\", \"Cabin__missing\"], plot_ax=next(axes_iter))\n",
    "Plotter.plot_decision_boundary(log_model, features=[\"Title_Mr\", \"Fare\"], plot_ax=next(axes_iter))\n",
    "Plotter.plot_decision_boundary(log_model, pca=True, plot_ax=next(axes_iter))\n",
    "\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "En termes pràctics i morals, ens interessa prioritzar el recall de la classe positiva (Sobreviu),\n",
    "ja que volem minimitzar falsos negatius i no deixar fora persones que realment sobreviuen.\n",
    "És a dir, optem per prioritzar en detectar tots els supervivents i ajustem el threshold si cal (baixant-lo,\n",
    "acceptant alguns FP per maximitzar la seguretat moral del nostre model abans que trobar-nos més FN.\n",
    "\n",
    "Per optimitzar la classificació del nostre cas, ens fixem especialment en:\n",
    "- Recall de la classe positiva → captar la majoria de supervivents.\n",
    "- Average Precision / F1-score → equilibrar que no hi hagi massa falsos positius."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Model Selection amb validació creuada (4 punts)\n",
    "\n",
    "Fent servir la mètrica trobada en l'apartat anterior, en aquest apartat caldrà seleccionar una sèrie de models i, fent ús de la validació creuada, seleccionar el millor model amb els seus respectius millors hyperpàrametres que haurem buscat fent hyperparameter search.\n",
    "\n",
    "La tasca d'aquesta pràctica s'enmarca dins l'aprenentatge computacional **supervisat**. A sklearn, disposem de diverses tècniques [(veure documentació)](https://scikit-learn.org/stable/supervised_learning.html). A les classes de teoria, hem vist tècniques com ara logistic regression, SVM amb diferents kernels, Nearest Neighbour... i també coneixeu altres tecniques d'altres cursos, com els arbres de decisio. Per aquest apartat es demana seleccionar **un minim de 3 models**.\n",
    "\n",
    "**Preguntes:**\n",
    "* Quins models heu considerat? Per què els heu seleccionat?\n",
    "* Fent servir validació creuada, escolliu el millor model (agafant els hiperparàmetres per defecte). Recordeu fer servir la mètrica utilitzada en l'apartat anterior i fer fer servir algun [tipus de validacio creuada](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "\n",
    "* Seleccioneu una sèrie d'hiperparàmetres a provar per cadascun dels models i realitzeu una cerca d'hiperparàmetres. Hi ha algun model que creieu que podeu descartar de primeres? Per què?\n",
    "\n",
    "* Mostreu els resultats en una taula on es mostri el model, els experiments realitzats i els resultats obtinguts (tant en train com en test). Podeu mostrar tambe el temps d'entrenament de cada model.\n",
    "\n",
    "* Quin tipus de validació heu escollit en la selecció de models?\n",
    "\n",
    "* Quines formes de buscar els millors hiperparàmetres heu trobat? Són costoses computacionalment parlant? [documentació](https://scikit-learn.org/stable/modules/grid_search.html). Quina heu seleccionat?\n",
    "\n",
    "* Si disposem de recursos limitats (per exemple, un PC durant 1 hora), quin dels mètodes creieu que obtindrà millor resultat final?\n",
    "\n",
    "* Opcional: Feu la prova, i amb el model i el metode de validació creuada escollit, configureu els diferents mètodes de cerca per a que s'executin durant el mateix temps (i.e. depenent del problema, 0,5h-1 hora). Analitzeu quin ha arribat a una millor solució. (Ajuda: estimeu el temps que trigarà a fer 1 training el vostre model, i així trobeu el número de intents que podeu fer en cada cas.)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Validació Creuada\n",
    "\n",
    "Comencem aquest secció parlant sobre validació creua"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    StratifiedKFold,\n",
    "    ShuffleSplit,\n",
    "    RepeatedStratifiedKFold,\n",
    "    cross_validate\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "KFOLD = \"K-Fold\"\n",
    "STRATIFIED = \"Stratified K-Fold\"\n",
    "REPEATED = \"Repeated Stratified K-Fold\"\n",
    "SHUFFLE = \"Shuffle & Split\"\n",
    "\n",
    "CROSS_VALIDATORS = [\n",
    "    KFOLD,\n",
    "    STRATIFIED,\n",
    "    REPEATED,\n",
    "    SHUFFLE,\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class CValidator:\n",
    "    @staticmethod\n",
    "    def cross_validate(model, method: str = \"kfold\", scoring: str = \"f1\", **kwargs):\n",
    "        X_full = pd.concat([model.X_train, model.X_test], axis=0)\n",
    "        y_full = pd.concat([model.y_train, model.y_test], axis=0)\n",
    "\n",
    "        cv_methods = {\n",
    "            KFOLD:         KFold(n_splits=kwargs.get(\"n_splits\", 5)),\n",
    "            STRATIFIED:    StratifiedKFold(n_splits=kwargs.get(\"n_splits\", 5)),\n",
    "            REPEATED:      RepeatedStratifiedKFold(n_splits=kwargs.get(\"n_splits\", 5), n_repeats=kwargs.get(\"n_repeats\", 3), random_state=model.random_state),\n",
    "            SHUFFLE:       ShuffleSplit(n_splits=kwargs.get(\"n_splits\", 5), test_size=kwargs.get(\"test_size\", 0.2), random_state=model.random_state),\n",
    "        }\n",
    "\n",
    "        if method not in cv_methods:\n",
    "            raise ValueError(f\"method must be one of: {list(cv_methods.keys())}\")\n",
    "\n",
    "        cv_splitter = cv_methods[method]\n",
    "\n",
    "        raw_results = cross_validate(\n",
    "            model.model_class(**model.hyperparameters),\n",
    "            X_full,\n",
    "            y_full,\n",
    "            cv=cv_splitter,\n",
    "            scoring=scoring,\n",
    "            return_train_score=True,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "\n",
    "        results = pd.DataFrame(raw_results)\n",
    "\n",
    "        summary = {\n",
    "            \"fit_time_mean\": results[\"fit_time\"].mean(),\n",
    "            \"score_time_mean\": results[\"score_time\"].mean(),\n",
    "            f\"train_score_mean\": results[f\"train_score\"].mean(),\n",
    "            f\"test_score_mean\": results[f\"test_score\"].mean(),\n",
    "            f\"overfit_gap\": results[f\"train_score\"].mean() - results[f\"test_score\"].mean(),\n",
    "        }\n",
    "\n",
    "        model.cv_results_ = pd.DataFrame([summary])\n",
    "\n",
    "        return model.cv_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for model_name, model_class in models.items():\n",
    "    log_model = Model(\n",
    "        model_class=model_class,\n",
    "        dataframe=DATAFRAME\n",
    "    )\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    with tqdm(CROSS_VALIDATORS, desc=\"Cross-validating model\", leave=False) as pbar:\n",
    "        for validator in pbar:\n",
    "            pbar.set_postfix({\"validator\": validator})\n",
    "            cv_result = CValidator.cross_validate(log_model, validator)\n",
    "            results[validator] = cv_result.to_dict(orient=\"records\")[0]\n",
    "\n",
    "    print(model_name)\n",
    "    display(pd.DataFrame.from_dict(results, orient=\"index\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Models"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Llibreries que necessitem en aquesta secció."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations, chain\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Els models que provarem son els següents:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "LOGISTIC_REGRESSION = \"Logistic Regression\"\n",
    "KNN = \"KNN\"\n",
    "GRADIENT_DESCENT = \"Gradient Descent\"\n",
    "RANDOM_FOREST = \"Random Forest\"\n",
    "EXTRA_TREES = \"Extra Trees\"\n",
    "SUPPORT_VECTOR_MACHINE = \"Support Vector Machine\"\n",
    "LINEAR_SVC = \"LinearSVC\"\n",
    "GRADIENT_BOOSTING = \"Gradient Boosting\"\n",
    "NAIVE_BAYES = \"Naive Bayes\"\n",
    "PERCEPTRON = \"Perceptron\"\n",
    "PASSIVE_AGGRESSIVE = \"Passive Aggressive\"\n",
    "NEURAL_NETWORK = \"Neural Network\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Per cada tipus de model, tenim la classe i alguns paràmetres que deixarem per defecte en cada cas. Més endavant, quan farem optimització d'hiperparàmetres trobaren els valors òptims en cada cas."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "models = {\n",
    "    LOGISTIC_REGRESSION: LogisticRegression,\n",
    "    KNN: KNeighborsClassifier,\n",
    "    GRADIENT_DESCENT: SGDClassifier,\n",
    "    RANDOM_FOREST: RandomForestClassifier,\n",
    "    EXTRA_TREES: ExtraTreesClassifier,\n",
    "    SUPPORT_VECTOR_MACHINE: SVC,\n",
    "    LINEAR_SVC: LinearSVC,\n",
    "    GRADIENT_BOOSTING: GradientBoostingClassifier,\n",
    "    NAIVE_BAYES: GaussianNB,\n",
    "    PERCEPTRON: Perceptron,\n",
    "    PASSIVE_AGGRESSIVE: PassiveAggressiveClassifier,\n",
    "    NEURAL_NETWORK: MLPClassifier\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Amb tot això, ja podem dur a terme un test bàsic evaluant cada model amb les mètriques triades."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "with tqdm(models.items(), desc=f\"Evaluating models\", leave=False) as pbar:\n",
    "    for model_name, model_class in pbar:\n",
    "        pbar.set_postfix({\"model\": model_name})\n",
    "\n",
    "        model = Model(\n",
    "            model_class=model_class,\n",
    "            dataframe=DATAFRAME,\n",
    "            name=model_name\n",
    "        )\n",
    "\n",
    "        results[model_name] = Metrics.evaluate(model)\n",
    "        Plotter.plot_roc(model=model, plot_ax=next(axes_iter))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "display(results_df)\n",
    "display(results_df.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "El dataframe encara té motles features (24), anem a veure si podem reduir aquestes dimensions a només les més importants. Fem un F-test i Chi-test per les variables numèriques i booleanes respectivament. D'aquesta manera, veurem si cada feature està relacionada amb la variable resposta."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = processed_df.drop(columns=[\"Survived\"])\n",
    "y = processed_df[\"Survived\"]\n",
    "\n",
    "numeric_features = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "boolean_features = X.select_dtypes(include=[\"bool\"]).columns\n",
    "\n",
    "anova_res = SelectKBest(f_classif, k='all').fit(X[numeric_features], y)\n",
    "chi2_res = SelectKBest(chi2, k='all').fit(X[boolean_features].astype(int), y)\n",
    "\n",
    "features_p_value = pd.DataFrame({\n",
    "    \"Feature\": list(numeric_features) + list(boolean_features),\n",
    "    \"Score\": list(anova_res.scores_) + list(chi2_res.scores_),\n",
    "    \"p-value\": list(anova_res.pvalues_) + list(chi2_res.pvalues_)\n",
    "}).sort_values(\"p-value\").reset_index(drop=True)\n",
    "\n",
    "display(features_p_value)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Les variables que tenen un $p-valor<0.05$ són estadísticament significatives però si ens fixem, hi ha 7 variables que tenen uns $p-valor$ molt més petits que la resta així que escollim aquestes 7."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "features_p_value_top = features_p_value[\"Feature\"].iloc[:7].tolist()\n",
    "print(features_p_value_top)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "D'aquestes set, observem que hi ha columnes que semblen que aporten la mateixa inforamció, com per exemple `Title_Mr` i `Sex_male`. Analitzem la matriu de correlació per trobar possibles redundàncies en les features."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corr_matrix = processed_df[features_p_value_top].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Matriu de correlació de les variables més importants (p-valor més baix)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Mirant la matriu de correlació podem observar que les variables Sex_male i Title_Mr estàn molt correlacionades i les variables Cabin_missing, Fare i Pclass també ho estàn bastant, així que prescindim de Title_Mr i Cabin_Missing.\n",
    "\n",
    "Ara amb aquestes 5 variables restants que hem vist que són molt significatives provem a fer diferents combinacions per a veure si podem predir correctament el model amb nomès dues variables."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "final_features = [v for v in features_p_value_top if v not in [\"Cabin__missing\", \"Title_Mr\"]]\n",
    "final_features"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Podem reduir tot aquest estudi de les features en un transformer."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def select_top_features(dataframe: pd.DataFrame, explanatory: str = \"Survived\", top_n=7, exclude_features: list = None):\n",
    "    exclude_features = exclude_features or []\n",
    "\n",
    "    y = dataframe[explanatory]\n",
    "    dataframe = dataframe.copy().drop(explanatory, axis=1)\n",
    "\n",
    "    num_fts = dataframe.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    bln_fts = dataframe.select_dtypes(include=[\"bool\"]).columns\n",
    "\n",
    "    anova_out = SelectKBest(f_classif, k=\"all\").fit(dataframe[num_fts], y)\n",
    "    chi2_out = SelectKBest(chi2, k=\"all\").fit(dataframe[bln_fts].astype(int), y)\n",
    "\n",
    "    p_vals = pd.DataFrame({\n",
    "        \"Feature\": list(num_fts) + list(bln_fts),\n",
    "        \"Score\": list(anova_out.scores_) + list(chi2_out.scores_),\n",
    "        \"p-value\": list(anova_out.pvalues_) + list(chi2_out.pvalues_)\n",
    "    }).sort_values(\"p-value\").reset_index(drop=True)\n",
    "\n",
    "    top_fts = p_vals[\"Feature\"].iloc[:top_n].tolist()\n",
    "    final_fts = [f for f in top_fts if f not in exclude_features]\n",
    "\n",
    "    return pd.concat([dataframe[final_fts], y], axis=1)\n",
    "\n",
    "feature_selector_transformer = FunctionTransformer(select_top_features, kw_args={\n",
    "    \"exclude_features\": [\"Cabin__missing\",\"Title_Mr\"]\n",
    "})"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I l'apliquem per obtenir un dataframe nou reduït."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "DATAFRAME_REDUCED = feature_selector_transformer.transform(DATAFRAME)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Amb aquest nou dataframe tenim relativament poques columnes, llavors podem iterar sobre totes les possibles combinacions de les features per trobar la millor combinació."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "with tqdm(models.items(), desc=\"Evaluating models\", leave=False) as pbar:\n",
    "    for model_name, model_class in pbar:\n",
    "        pbar.set_postfix({\"model\": model_name})\n",
    "        model = Model(\n",
    "            model_class=model_class,\n",
    "            dataframe=DATAFRAME_REDUCED,\n",
    "            name=model_name\n",
    "        )\n",
    "\n",
    "        results[model_name] = Metrics.evaluate(model)\n",
    "        Plotter.plot_roc(model=model, plot_ax=next(axes_iter))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "display(results_df)\n",
    "display(results_df.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "for model_name, model_class in tqdm(models.items(), desc=\"Evaluating models\"):\n",
    "    model = Model(\n",
    "        model_class=model_class,\n",
    "        dataframe=DATAFRAME_REDUCED,\n",
    "        name=model_name\n",
    "    )\n",
    "\n",
    "    results[model_name] = Metrics.evaluate(model)\n",
    "    Plotter.plot_roc(model=model, plot_ax=next(axes_iter))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
    "display(results_df)\n",
    "display(results_df.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T18:41:06.860213Z",
     "start_time": "2025-10-26T18:40:50.655235Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "Només amb aquest canvi, aconseguim pujar totes les mètriques de gairebé tots els models.\n",
    "\n",
    "Amb aquest test veiem que és important quines features considerem, així que fem un test que provi totes les combinacions possibles per triar les que millor funcionen."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cols = DATAFRAME_REDUCED.drop(EXPLANATORY, axis=1).columns\n",
    "features_combinations = []\n",
    "for r in range(1, len(cols)+1):\n",
    "    for combo in combinations(cols, r):\n",
    "        features_combinations.append(list(combo))\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name, model_class in models.items():\n",
    "    model = Model(\n",
    "        model_class=model_class,\n",
    "        dataframe=DATAFRAME_REDUCED,\n",
    "        name=model_name\n",
    "    )\n",
    "\n",
    "    with tqdm(features_combinations, desc=f\"{model_name}\", leave=False) as pbar:\n",
    "        for features in pbar:\n",
    "            pbar.set_postfix({\"features\": features})\n",
    "\n",
    "            model.update_dataset(features=features)\n",
    "            metrics = Metrics.evaluate(model)\n",
    "            results.append({\"Model\": model_name, \"Features\": features, **metrics})\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "display(results_df.describe())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T18:41:16.477938Z",
     "start_time": "2025-10-26T18:41:16.464685Z"
    }
   },
   "cell_type": "markdown",
   "source": "Mirem les combinacions que millor score han aconseguit per cada model:"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T12:51:05.353074Z",
     "start_time": "2025-10-26T12:51:05.313353Z"
    }
   },
   "cell_type": "markdown",
   "source": "Per raons de visualització, representem les combinacions de dues features que millors resultats han obtingut."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T18:41:21.762484Z",
     "start_time": "2025-10-26T18:41:18.157493Z"
    }
   },
   "cell_type": "markdown",
   "source": "Per raons de visualització, representem les combinacions de dues features que millors resultats han obtingut."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(22, 16))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "with tqdm(models.items(), desc=\"Evaluating models\", leave=False) as pbar:\n",
    "    for model_name, model_class in pbar:\n",
    "        pbar.set_postfix({\"model\": model_name})\n",
    "        filtered = results_df[\n",
    "            (results_df[\"Model\"] == model_name) &\n",
    "            (results_df[\"Features\"].apply(lambda x: len(x) == 2))\n",
    "        ]\n",
    "\n",
    "        top_model = filtered.sort_values(by=\"F1\", ascending=False).iloc[0]\n",
    "        features = top_model[\"Features\"]\n",
    "\n",
    "        model = Model(\n",
    "            model_class=model_class,\n",
    "            dataframe=DATAFRAME_REDUCED,\n",
    "            name=model_name\n",
    "        )\n",
    "        model.update_dataset(features=features)\n",
    "        Plotter.plot_decision_boundary(\n",
    "            model=model,\n",
    "            plot_ax=next(axes_iter)\n",
    "        )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Per aquests gràfics, detectem un problema molt gran: l'overfitting. En el cas del Random Forest (i Extra Trees) veiem com el model ha començat a memoritzar cada dada. A nivell de model, el que està passant és que està creant una branca per a cada dada. Això passa perquè el paràmetre `max_depth` per defecte és `None`, és a dir que pot créixer l'arbre amb tants nivells com vulgui. Amb les poquees dades que tenim (menys de 1000), això significarà que ràpidament es crearan fulles per cada una d'elles.\n",
    "\n",
    "Per aquesta raó, el següent pas és trobar els hiperparàmetres òptims: millorin el rendiment i també evitin l'overfitting."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HyperParameterOptimization:\n",
    "    @staticmethod\n",
    "    def grid_search(model: Model, param_grid: dict, scoring: str = \"f1\", cv = 5):\n",
    "        return HyperParameterOptimization.search(\n",
    "            model=model,\n",
    "            searcher_class=GridSearchCV,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=cv\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def randomized_search(model: Model, param_grid: dict, scoring: str = \"f1\", cv = 5, n_iter: int = 20):\n",
    "        return HyperParameterOptimization.search(\n",
    "            model=model,\n",
    "            searcher_class=RandomizedSearchCV,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            n_iter=n_iter,\n",
    "            random_state=model.random_state,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def randomized_search(model: Model, param_grid: dict, scoring: str = \"f1\", cv = 5, factor: int = 2):\n",
    "        return HyperParameterOptimization.search(\n",
    "            model=model,\n",
    "            searcher_class=HalvingGridSearchCV,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            factor=factor,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def search(model: Model, searcher_class, param_grid: dict, scoring: str = \"f1\", cv = 5, **kwargs):\n",
    "        model_class = model.model_class(**model.hyperparameters)\n",
    "        searcher = searcher_class(\n",
    "            estimator=model_class,\n",
    "            param_grid=param_grid,\n",
    "            scoring=scoring,\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            return_train_score=True,\n",
    "            error_score=\"nan\",\n",
    "            **kwargs\n",
    "        )\n",
    "        searcher.fit(model.X_train, model.y_train)\n",
    "        return HyperParameterOptimization.update_model(model, searcher)\n",
    "\n",
    "    @staticmethod\n",
    "    def update_model(model: Model, searcher):\n",
    "        model.model_class = searcher.best_estimator_\n",
    "        model.hyperparameters = searcher.best_params_\n",
    "        model.model_class.fit(model.X_train, model.y_train)\n",
    "        model.valid_pred = False\n",
    "        model.search_results_ = pd.DataFrame(searcher.cv_results_)\n",
    "        model.search_results_[\"overfit_gap\"] = model.search_results_[\"mean_train_score\"] - model.search_results_[\"mean_test_score\"]\n",
    "\n",
    "        return model.search_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class HyperParameterOptimization:\n",
    "    @staticmethod\n",
    "    def hyperparameter_search(model: Model, param_grid, search_type=\"grid\", scoring=\"f1\", cv=5, n_iter=20, factor=2):\n",
    "        model_class = model.model_class(**model.hyperparameters)\n",
    "\n",
    "        if search_type == \"grid\":\n",
    "            searcher = GridSearchCV(model_class, param_grid, scoring=scoring, cv=cv, n_jobs=-1, return_train_score=True)\n",
    "        elif search_type == \"random\":\n",
    "            searcher = RandomizedSearchCV(model_class, param_distributions=param_grid, n_iter=n_iter, scoring=scoring, cv=cv, n_jobs=-1,\n",
    "                                          random_state=model.random_state, return_train_score=True)\n",
    "        elif search_type == \"halving\":\n",
    "            searcher = HalvingGridSearchCV(model_class, param_grid, scoring=scoring, cv=cv, factor=factor, n_jobs=-1, return_train_score=True)\n",
    "        else:\n",
    "            raise ValueError(\"search_type must be 'grid', 'random', or 'halving'\")\n",
    "\n",
    "        searcher.fit(model.X_train, model.y_train)\n",
    "        model.model_class = searcher.best_estimator_\n",
    "        model.hyperparameters = searcher.best_params_\n",
    "        model.model_class.fit(model.X_train, model.y_train)\n",
    "        model.valid_pred = False\n",
    "\n",
    "        model.search_results_ = pd.DataFrame(searcher.cv_results_)\n",
    "        model.search_results_[\"overfit_gap\"] = model.search_results_[\"mean_train_score\"] - model.search_results_[\"mean_test_score\"]\n",
    "\n",
    "        return model.search_results_"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T16:45:20.308779Z",
     "start_time": "2025-10-26T16:45:20.302891Z"
    }
   },
   "cell_type": "markdown",
   "source": "Ja sabem que la mètrica important que farem servir és l'F1 score. Per poder visualitzar el rendiment de cada parella de paràmetres, afegim un nou mètode a la class existen `Plotter`, que representi aquest score en un mapa de calor en funció de dos paràmetres."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_param_heatmap(model: Model, param_x, param_y, plot_ax=None, metric=\"mean_test_score\"):\n",
    "    if not hasattr(model, \"search_results_\") or model.search_results_ is None:\n",
    "        raise ValueError(\"No search results found. Run HyperParameterOptimization first.\")\n",
    "\n",
    "    df = model.search_results_\n",
    "\n",
    "    param_x_col = f\"param_{param_x}\"\n",
    "    param_y_col = f\"param_{param_y}\"\n",
    "\n",
    "    if param_x_col not in df.columns or param_y_col not in df.columns:\n",
    "        raise KeyError(f\"Parameters not found in search results. Available: {[c for c in df.columns if c.startswith('param_')]}\")\n",
    "\n",
    "    pivot = df.pivot_table(values=metric, index=param_y_col, columns=param_x_col)\n",
    "\n",
    "    if plot_ax is None:\n",
    "        _, plot_ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "    title = f\"{model.name}: {metric} Heatmap\"\n",
    "\n",
    "    sns.heatmap(pivot, annot=pivot.size <= 25, fmt=\".4f\", cmap=\"coolwarm\", ax=plot_ax)\n",
    "    plot_ax.set_title(title)\n",
    "    plot_ax.set_xlabel(param_x)\n",
    "    plot_ax.set_ylabel(param_y)\n",
    "\n",
    "Plotter.plot_param_heatmap = staticmethod(plot_param_heatmap)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rf_model = Model(\n",
    "    model_class=RandomForestClassifier,\n",
    "    dataframe=DATAFRAME\n",
    ")\n",
    "\n",
    "param_grid = {\"n_estimators\": [1, 100, 1000], \"max_depth\": [1, 100, 1000]}\n",
    "results = HyperParameterOptimization.grid_search(rf_model, param_grid)\n",
    "display(rf_model.hyperparameters)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rf_model = Model(\n",
    "    model_class=RandomForestClassifier,\n",
    "    dataframe=DATAFRAME\n",
    ")\n",
    "\n",
    "param_grid = {\"n_estimators\": [1, 100, 1000], \"max_depth\": [1, 100, 1000]}\n",
    "results = HyperParameterOptimization.hyperparameter_search(rf_model, param_grid)\n",
    "display(rf_model.hyperparameters)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(22, 5))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "Plotter.plot_param_heatmap(rf_model, \"n_estimators\", \"max_depth\", plot_ax=next(axes_iter))\n",
    "Plotter.plot_param_heatmap(rf_model, \"n_estimators\", \"max_depth\", plot_ax=next(axes_iter), metric=\"overfit_gap\")\n",
    "\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rf_model = Model(\n",
    "    model_class=RandomForestClassifier,\n",
    "    dataframe=DATAFRAME\n",
    ")\n",
    "\n",
    "param_grid = {\"n_estimators\": range(1, 502, 50), \"max_depth\": range(1, 11)}\n",
    "results = HyperParameterOptimization.grid_search(rf_model, param_grid)\n",
    "display(rf_model.hyperparameters)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 7))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "Plotter.plot_param_heatmap(rf_model, \"n_estimators\", \"max_depth\", plot_ax=next(axes_iter))\n",
    "Plotter.plot_param_heatmap(rf_model, \"n_estimators\", \"max_depth\", plot_ax=next(axes_iter), metric=\"overfit_gap\")\n",
    "\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rf_model = Model(\n",
    "    model_class=RandomForestClassifier,\n",
    "    dataframe=DATAFRAME\n",
    ")\n",
    "\n",
    "param_grid = {\"n_estimators\": range(1, 502, 50), \"max_depth\": range(1, 11)}\n",
    "results = HyperParameterOptimization.hyperparameter_search(rf_model, param_grid)\n",
    "display(rf_model.hyperparameters)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 7))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "Plotter.plot_param_heatmap(rf_model, \"n_estimators\", \"max_depth\", plot_ax=next(axes_iter))\n",
    "Plotter.plot_param_heatmap(rf_model, \"n_estimators\", \"max_depth\", plot_ax=next(axes_iter), metric=\"overfit_gap\")\n",
    "\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T16:04:08.683219Z",
     "start_time": "2025-10-26T16:04:08.677309Z"
    }
   },
   "cell_type": "markdown",
   "source": "Pel segon gràfic observem que pujar de les 4 capes (tree splits) fa que la diferència entre train i test sigui considerable (> 0.05). Veiem que el nombre d'estimadors gairebé no influeix en el overfit del model, simplement fa que sigui més estable, observem el gràfic:"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "rf_model.search_results_.columns",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(22, 7))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "Plotter.plot_param_heatmap(rf_model, \"n_estimators\", \"max_depth\", plot_ax=next(axes_iter), metric=\"std_train_score\")\n",
    "Plotter.plot_param_heatmap(rf_model, \"n_estimators\", \"max_depth\", plot_ax=next(axes_iter), metric=\"mean_fit_time\")\n",
    "\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "param_grids = {\n",
    "    LOGISTIC_REGRESSION: {\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"max_iter\": [500, 1000, 2000],\n",
    "        \"solver\": [\"liblinear\", \"saga\"]\n",
    "    },\n",
    "    KNN: {\n",
    "        \"n_neighbors\": range(1,15, 2),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"metric\": [\"euclidean\", \"manhattan\"],\n",
    "        \"n_jobs\": [-1],\n",
    "    },\n",
    "    GRADIENT_DESCENT: {\n",
    "        \"loss\": [\"hinge\", \"log_loss\", \"modified_huber\", \"squared_hinge\"],\n",
    "        \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        \"penalty\": [\"l2\", \"l1\", \"elasticnet\", None],\n",
    "        \"max_iter\": [500, 1000, 2000],\n",
    "        \"learning_rate\": [\"constant\", \"optimal\", \"adaptive\", \"invscaling\"],\n",
    "        \"eta0\": [0.001, 0.01, 0.1]\n",
    "    },\n",
    "    RANDOM_FOREST: {\n",
    "        \"n_estimators\": range(1, 500, 50),\n",
    "        \"max_depth\": [2, 3, 4, 5, 10, 15, None],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"n_jobs\": [-1],\n",
    "    },\n",
    "    EXTRA_TREES: {\n",
    "        \"n_estimators\": range(1, 500, 50),\n",
    "        \"max_depth\": [2, 3, 4, 5, 10, 15, None],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"n_jobs\": [-1],\n",
    "    },\n",
    "    SUPPORT_VECTOR_MACHINE: {\n",
    "        \"C\": [0.001, 0.1, 1, 10, 100],\n",
    "        \"kernel\": [\"linear\", \"rbf\", \"poly\", \"sigmoid\"],\n",
    "        \"gamma\": [\"scale\", \"auto\"]\n",
    "    },\n",
    "    LINEAR_SVC: {\n",
    "        \"C\": [0.01, 0.1, 1, 10, 100],\n",
    "        \"loss\": [\"hinge\", \"squared_hinge\"],\n",
    "        \"max_iter\": [1000, 2000, 5000],\n",
    "    },\n",
    "    GRADIENT_BOOSTING: {\n",
    "        \"n_estimators\": range(1, 500, 50),\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"max_depth\": [2, 3, 4, 5, 6],\n",
    "        \"subsample\": [0.6, 0.7, 0.8, 1.0],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\", None],\n",
    "    },\n",
    "    NAIVE_BAYES: {\n",
    "        \"var_smoothing\": [1e-12, 1e-11, 1e-10, 1e-9, 1e-8, 1e-7, 1e-6, 1e-5],\n",
    "    },\n",
    "    PERCEPTRON: {\n",
    "        \"alpha\": [1e-5, 1e-4, 1e-3, 1e-2],\n",
    "        \"penalty\": [None, \"l2\", \"l1\", \"elasticnet\"],\n",
    "        \"max_iter\": [500, 1000, 2000],\n",
    "        \"eta0\": [0.001, 0.01, 0.1],\n",
    "        \"learning_rate\": [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"]\n",
    "    },\n",
    "    PASSIVE_AGGRESSIVE: {\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 10],\n",
    "        \"max_iter\": [500, 1000, 2000],\n",
    "        \"loss\": [\"hinge\", \"squared_hinge\"]\n",
    "    },\n",
    "    NEURAL_NETWORK: {\n",
    "        \"hidden_layer_sizes\": [(50,), (100,), (50,50), (100,50), (50,100,50)],\n",
    "        \"activation\": [\"relu\", \"tanh\", \"logistic\"],\n",
    "        \"alpha\": [1e-5, 1e-4, 0.001, 0.01],\n",
    "        \"learning_rate\": [\"constant\", \"adaptive\", \"invscaling\"],\n",
    "        \"learning_rate_init\": [0.001, 0.01, 0.1],\n",
    "        \"max_iter\": [500, 1000, 2000],\n",
    "        \"solver\": [\"adam\", \"sgd\"]\n",
    "    }\n",
    "}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Amb aquest diccionari simplement iterem sobre tots els models i guardem els resultats."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "specific_model = None\n",
    "\n",
    "results = {}\n",
    "opt_params = {}\n",
    "opt_models = {}\n",
    "\n",
    "with tqdm(models.items(), desc=\"Optimizing models\", leave=False) as pbar:\n",
    "    for model_name, model_class in pbar:\n",
    "        if (specific_model is not None) and (specific_model != model_name):\n",
    "            continue\n",
    "        pbar.set_postfix({\"model\": model_name})\n",
    "\n",
    "        model = Model(\n",
    "            model_class=model_class,\n",
    "            dataframe=DATAFRAME_REDUCED\n",
    "        )\n",
    "\n",
    "        param_grid = param_grids[model_name]\n",
    "\n",
    "        results[model_name] = HyperParameterOptimization.grid_search(model, param_grid)\n",
    "        opt_params[model_name] = model.hyperparameters\n",
    "        opt_models[model_name] = model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = opt_models[KNN]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 7))\n",
    "axes_iter = iter(axes.flatten())\n",
    "\n",
    "Plotter.plot_param_heatmap(model, \"n_neighbors\", \"weights\", plot_ax=next(axes_iter))\n",
    "Plotter.plot_param_heatmap(model, \"n_neighbors\", \"weights\", plot_ax=next(axes_iter), metric=\"overfit_gap\")\n",
    "\n",
    "plt.tight_layout()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "display(opt_params)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:56:31.069943Z",
     "start_time": "2025-10-26T15:56:30.723537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "results = {}\n",
    "\n",
    "with tqdm(models.items(), desc=\"Optimizing models\", leave=False) as pbar:\n",
    "    for model_name, model_class in pbar:\n",
    "        pbar.set_postfix({\"model\": model_name})\n",
    "\n",
    "        param_grid = param_grids[model_name]\n",
    "        print(param_grid)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 3\u001B[39m\n\u001B[32m      1\u001B[39m results = {}\n\u001B[32m----> \u001B[39m\u001B[32m3\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43mtqdm\u001B[49m(models.items(), desc=\u001B[33m\"\u001B[39m\u001B[33mOptimizing models\u001B[39m\u001B[33m\"\u001B[39m, leave=\u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[32m      4\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m model_name, model_class \u001B[38;5;129;01min\u001B[39;00m pbar:\n\u001B[32m      5\u001B[39m         pbar.set_postfix({\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m: model_name})\n",
      "\u001B[31mNameError\u001B[39m: name 'tqdm' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Anàlisi Final (1.5 punt)\n",
    "\n",
    "Un cop seleccionat el millor model amb els millors hiperparàmetres, caldrà fer un anàlisi final amb els resultats obtinguts.\n",
    "\n",
    "**Preguntes:**\n",
    "* Mostreu les corves ROC/PR (la que hagueu escollit en l'apartat 2) i interpreteu els resultats.\n",
    "\n",
    "* Analitzeu en detall les diferents mètriques que trobeu adients i comenteu per sobre com podrieu fer servir aquest model en un futur. Això és el que es coneix com un cas d'ús.\n",
    "\n",
    "* Com creieu que es podria millorar el vostre model?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
