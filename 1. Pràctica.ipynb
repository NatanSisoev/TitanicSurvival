{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZMnmdDBCl7F"
      },
      "source": [
        "# Pràctica 1: Resolem un problema de classificació\n",
        "\n",
        "## Objectius\n",
        "\n",
        "Els objectius d'aquesta pràctica són:\n",
        "\n",
        "* Aplicar els coneixements adquirits sobre processament de dades, classificació i validacio creuada.\n",
        "  \n",
        "* Ser capaç de comparar diferents models de classificació.\n",
        "\n",
        "* Ser capac de fer cerca d'hiperparàmetres.\n",
        "\n",
        "* Entendre i implementar la validació creuada.\n",
        "\n",
        "* Analitzar detalladament els resultats obtinguts durant l'aprenentatge dels diferents models.\n",
        "\n",
        "Aquesta pràctica és prèvia al cas kaggle que realitzareu durant la segona part de l'assignatura. En aquesta primera pràctica les preguntes estan definides, però us ha de servir d'aprenentatge a l'hora de saber com estructurar un projecte d'aprenentatge automàtic ja que en el cas kaggle no tindreu les preguntes.\n",
        "\n",
        "## Base de dades\n",
        "\n",
        "En aquesta pràctica farem servir la base de dades del titanic. L'atribut que predirem es Survived, el qual ens diu si cada passatger va sobreviure o no.\n",
        "\n",
        "\n",
        "https://www.kaggle.com/c/titanic/data\n",
        "\n",
        "\n",
        "## Treball en grup\n",
        "Aquesta pràctica es treballarà en grups de 2-3 persones. En casos excepcionals i degudament justificats la pràctica es podrà realitzar de forma individual.\n",
        "\n",
        "## Seguiment i entrega de la pràctica\n",
        "\n",
        "En la pràctica 1 es presenten diverses tasques per fer una correcta comparativa dels resultats obtinguts per diversos mètodes de classificació en una mateixa base de dades.\n",
        "\n",
        "En aquesta pràctica es realitzaran sessions de seguiment del treball. Aquestes sessions de treball estan orientades a que els alumnes que vingueu pugueu preguntar i resoldre dubtes sobre les dades, preguntar sobre l'objectiu de cada apartat dels enunciats que no us hagi quedat clar, i preguntar sobre els resultats que esteu obtenint a l'hora d'analitzar les dades. És molt recomanable venir a classe amb el treball fet per tal de poder comentar dubtes.\n",
        "\n",
        "Pel que fa l'entrega, caldrà entregar per caronte el següent:\n",
        "\n",
        "1. Memòria en format PDF explicant els resultats trobats sobre la bases de dades. La memòria ha d'utilitzar la plantilla de LaTeX que podeu trobar al Caronte i ha de ser de com a màxim 3 pàgines.\n",
        "   \n",
        "2. Notebook amb el respectiu codi de python.\n",
        "\n",
        "3. (Opcional) Presentació amb els resultats 4 min màxim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zdgsxfuCl7S"
      },
      "source": [
        "# Descripció de la pràctica\n",
        "\n",
        "A continuació es mostren tots els continguts que s'evaluaran en la pràctica:\n",
        "\n",
        "1. EDA (exploratory data analysis) (1 punt):\n",
        "  * Anàlisi de tamany i tipologia de dades\n",
        "  * Primera valoració de correlacions\n",
        "  * Anàlisi atribut target\n",
        "2. Preprocessing (2 punts):\n",
        "  * Eliminació de nans\n",
        "  * Encoding de categòriques\n",
        "  * Altres (PCA, normalització, ...)\n",
        "3. Metric selection (1.5 punts):\n",
        "  * Selecció de la millor mètrica pel problema\n",
        "  * Visualització de ROC/AUC per model base\n",
        "4. Model Selection amb Crossvalidation (4 punts):\n",
        "  * Selecció del millor model\n",
        "  * Cerca d'hiperparàmetres\n",
        "5. Anàlisi final (1.5 punt)\n",
        "\n",
        "La pràctica esta construida a partir d'un seguit de preguntes orientatives en cada apartat les quals tenen relació amb els continguts evaluables. **NO cal contestar-les totes**. Són una guia per a que reflexioneu i aprengueu detalls de cada apartat. És recomanable llegir totes les preguntes abans de realitzar la pràctica i tenir-les en ment a l'hora d'executar-la.\n",
        "\n",
        "\n",
        "**IMPORTANT**: El que es valorarà en la pràctica és la capacitat de mantenir una narrativa coherent alhora que s'expliquen els resultats. No es mirarà tant que alguna pregunta quedi per respondre sinó que els passos seguits en base als resultats obtinguts siguin coherents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuENGdYFCl7S"
      },
      "source": [
        "### 1. EDA (exploratory data analysis) (1 punt)\n",
        "\n",
        "Abans de res cal sempre veure com es la base de dades assignada.\n",
        "\n",
        "**Preguntes:**\n",
        "* Quants atributs té la vostra base de dades?\n",
        "* Quin tipus d'atributs teniu? (Númerics, temporals, categòrics, binaris...)\n",
        "* Com es el target? quantes categories diferents existeixen?\n",
        "* Tenim nans en les dades?\n",
        "* Podeu veure alguna correlació entre X i y?\n",
        "* Estan balancejades les etiquetes (distribució similar entre categories)? Creieu que pot afectar a la classificació la seva distribució?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nombre d'atributs: 12\n",
            "\n",
            "Tipus d'atributs:\n",
            " PassengerId      int64\n",
            "Survived         int64\n",
            "Pclass           int64\n",
            "Name            object\n",
            "Sex             object\n",
            "Age            float64\n",
            "SibSp            int64\n",
            "Parch            int64\n",
            "Ticket          object\n",
            "Fare           float64\n",
            "Cabin           object\n",
            "Embarked        object\n",
            "dtype: object\n",
            "\n",
            "Target: Survived\n",
            "   Nombre de categories: 2\n",
            "   Categories existents: [0 1]\n",
            "\n",
            "Nombre de NaNs per columna:\n",
            " PassengerId      0\n",
            "Survived         0\n",
            "Pclass           0\n",
            "Name             0\n",
            "Sex              0\n",
            "Age            177\n",
            "SibSp            0\n",
            "Parch            0\n",
            "Ticket           0\n",
            "Fare             0\n",
            "Cabin          687\n",
            "Embarked         2\n",
            "dtype: int64\n",
            "\n",
            "Correlació de cada atribut numèric amb el target:\n",
            " Survived       1.000000\n",
            "Fare           0.257307\n",
            "Parch          0.081629\n",
            "PassengerId   -0.005007\n",
            "SibSp         -0.035322\n",
            "Age           -0.077221\n",
            "Pclass        -0.338481\n",
            "Name: Survived, dtype: float64\n",
            "\n",
            "Distribució de les etiquetes (proporció):\n",
            " Survived\n",
            "0    0.616162\n",
            "1    0.383838\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Carregar el dataset\n",
        "df = pd.read_csv(\"./titanic/train.csv\")\n",
        "\n",
        "# 1️⃣ Quants atributs té la base de dades?\n",
        "num_atributs = df.shape[1]\n",
        "print(\"Nombre d'atributs:\", num_atributs)\n",
        "\n",
        "# 2️⃣ Tipus d'atributs\n",
        "tipus_atributs = df.dtypes\n",
        "print(\"\\nTipus d'atributs:\\n\", tipus_atributs)\n",
        "\n",
        "# 3️⃣ Target i categories\n",
        "target = 'Survived'\n",
        "categories_target = df[target].nunique()\n",
        "print(\"\\nTarget:\", target)\n",
        "print(\"   Nombre de categories:\", categories_target)\n",
        "print(\"   Categories existents:\", df[target].unique())\n",
        "\n",
        "# 4️⃣ NaNs en les dades\n",
        "nans = df.isna().sum()\n",
        "print(\"\\nNombre de NaNs per columna:\\n\", nans)\n",
        "\n",
        "# 5️⃣ Correlació entre atributs numèrics i target\n",
        "# Seleccionar només columnes numèriques\n",
        "num_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# Correlació només amb numèrics\n",
        "correlacions = df[num_cols].corr()[target].sort_values(ascending=False)\n",
        "print(\"\\nCorrelació de cada atribut numèric amb el target:\\n\", correlacions)\n",
        "\n",
        "# 6️⃣ Distribució de les etiquetes\n",
        "distribucio_target = df[target].value_counts(normalize=True)\n",
        "print(\"\\nDistribució de les etiquetes (proporció):\\n\", distribucio_target)\n",
        "\n",
        "# Observació de l'impacte: es pot veure que no està balancejat\n",
        "if distribucio_target.min() / distribucio_target.max() < 0.5:\n",
        "    print(\"   ⚠️ Les etiquetes no estan balancejades, pot afectar la classificació\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuLtWQWpCl7T"
      },
      "source": [
        "### 2. Preprocessing (normalitzation, outlier removal, feature selection, ...) (2 punts)\n",
        "Un cop vistes les dades de les que es disposa, cal preparar les dades per als nostres algoritmes. Segons la tipologia de dades, es poden filtrar atributs, aplicar-hi reductors de dimensionalitat, codificar categories textuals en valors numèrics, normalitzar les dades, treure outliers...\n",
        "\n",
        "Navegueu per la [documentació de sklearn sobre preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) per tal de trobar les diferents opcions que proporciona sklearn.\n",
        "\n",
        "**Preguntes:**\n",
        "* Estan les dades normalitzades? Caldria fer-ho?\n",
        "* En cas que les normalitzeu, quin tipus de normalització serà més adient per a les vostres dades?\n",
        "* Teniu gaires dades sense informació (nans)? Tingueu en compte que hi ha metodes que no els toleren durant l'aprenentatge. Com afecta a la classificació si les filtrem? I si les reompliu? Com ho farieu? [Pista](https://scikit-learn.org/stable/modules/impute.html)\n",
        "* Teniu dades categòriques? Quina seria la codificació amb més sentit?\n",
        "* Podreu treure algun atribut extra de les categòriques (per exemple, aplicant alguna regla sobre el text)?\n",
        "* Caldria aplicar PCA? Quins beneficis o inconvenients trobarieu?\n",
        "* Caldria aplicar alguna tècnica de selecció de variables? Ho trobeu necessari?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "APARTAT 2: PREPROCESSING\n",
            "================================================================================\n",
            "\n",
            "📊 2.1 ANÀLISI DE VALORS FALTANTS\n",
            "----------------------------------------\n",
            "          Total_NaN  Percentatge\n",
            "Age             177        19.87\n",
            "Cabin           687        77.10\n",
            "Embarked          2         0.22\n",
            "\n",
            "⚠️  ESTRATÈGIA DE TRACTAMENT DE NaNs:\n",
            "\n",
            "1. Age (19.87% NaNs):\n",
            "   → Estratègia: KNNImputer amb k=5\n",
            "   → Justificació: L'edat és una variable crítica per la supervivència.\n",
            "     KNNImputer utilitza la informació multivariable (Sex, Pclass, Fare, etc.)\n",
            "     per estimar valors més realistes que una simple mitjana.\n",
            "     Mantenim la variabilitat natural i les relacions amb altres variables.\n",
            "\n",
            "2. Cabin (77.10% NaNs):\n",
            "   → Estratègia: Crear variable binària 'Cabin_known' + eliminar columna original\n",
            "   → Justificació: Massa NaNs per imputar de manera fiable.\n",
            "     Tenir cabina coneguda pot indicar classe social o proximitat a les sortides.\n",
            "     La informació binària (té/no té cabina) és més útil que la cabina específica.\n",
            "\n",
            "3. Embarked (0.22% NaNs):\n",
            "   → Estratègia: SimpleImputer amb strategy='most_frequent'\n",
            "   → Justificació: Percentatge mínim de NaNs. La moda és suficient i no\n",
            "     distorsiona la distribució. És eficient i manté la consistència.\n",
            "\n",
            "🔍 ALTERNATIVES CONSIDERADES I DESCARTADES:\n",
            "\n",
            "• SimpleImputer (mean/median/mode):\n",
            "  ✗ Per Age: No considera relacions amb altres variables (Sex, Pclass, etc.)\n",
            "  ✓ Per Embarked: Adequat pel baix % de NaNs\n",
            "\n",
            "• IterativeImputer (MICE):\n",
            "  ✗ Massa computacionalment costós per aquest dataset\n",
            "  ✗ Risc de sobreajust amb poques dades\n",
            "  ✗ KNNImputer ja captura relacions multivariables de forma més simple\n",
            "\n",
            "• Eliminar files amb NaNs:\n",
            "  ✗ Perdríem ~20% del dataset (177 passatgers)\n",
            "  ✗ Pèrdua significativa d'informació per entrenar el model\n",
            "\n",
            "• MissingIndicator:\n",
            "  ✗ Redundant: ja creem 'Cabin_known' per capturar aquesta informació\n",
            "\n",
            "✓ CONCLUSIÓ: KNNImputer per Age + Cabin_known + most_frequent per Embarked\n",
            "\n",
            "🔧 2.2 FEATURE ENGINEERING\n",
            "----------------------------------------\n",
            "\n",
            "📌 2.2.1 EXTRACCIÓ DE TÍTOL DEL NOM\n",
            "Títols únics trobats: 17\n",
            "\n",
            "Distribució original de títols:\n",
            "Title\n",
            "Mr          517\n",
            "Miss        182\n",
            "Mrs         125\n",
            "Master       40\n",
            "Dr            7\n",
            "Rev           6\n",
            "Col           2\n",
            "Mlle          2\n",
            "Major         2\n",
            "Ms            1\n",
            "Mme           1\n",
            "Don           1\n",
            "Lady          1\n",
            "Sir           1\n",
            "Capt          1\n",
            "Countess      1\n",
            "Jonkheer      1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "✓ Títols agrupats:\n",
            "Title\n",
            "Mr         517\n",
            "Miss       185\n",
            "Mrs        128\n",
            "Master      40\n",
            "Officer     21\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Lògica d'agrupació:\n",
            "  • Miss: Dones joves/solteres (Miss, Mlle, Ms)\n",
            "  • Mrs: Dones casades/classe alta (Mrs, Mme, Lady, Countess, Dona)\n",
            "  • Officer: Homes amb títols oficials/militars/aristocràtics\n",
            "  • Mr, Master: Mantinguts per alta freqüència i rellevància\n",
            "\n",
            "📌 2.2.2 CREACIÓ DE CABIN_KNOWN\n",
            "✓ 204 passatgers (22.90%) amb cabina coneguda\n",
            "   Justificació: Tenir cabina pot correlacionar amb classe social i supervivència\n",
            "\n",
            "📌 2.2.3 CREACIÓ DE FAMILY_SIZE\n",
            "✓ Rang de Family_size: [1, 11]\n",
            "   Distribució: {1: 537, 2: 161, 3: 102, 4: 29, 5: 15, 6: 22, 7: 12, 8: 6, 11: 7}\n",
            "   Justificació: Famílies grans poden tenir més dificultat per evacuar\n",
            "\n",
            "📌 2.2.4 CREACIÓ DE IS_ALONE\n",
            "✓ 537 passatgers (60.27%) viatjant sols\n",
            "   Justificació: Viatjar sol pot afectar les possibilitats de supervivència\n",
            "\n",
            "📌 2.2.5 SELECCIÓ DE FEATURES - ANÀLISI\n",
            "\n",
            "🔍 ENFOCAMENT: Mantenir totes les variables (excepte les eliminades explícitament)\n",
            "\n",
            "RAONAMENT:\n",
            "  • En ML no necessitem causalitat, sinó CORRELACIÓ/ASSOCIACIÓ\n",
            "  • Prioritzem ACCURACY sobre eficiència computacional\n",
            "  • Sense conèixer la naturalesa exacta de totes les dades, és arriscat eliminar-les\n",
            "\n",
            "✓ Variables mantingudes: Pclass, Sex, Age, SibSp, Parch, Fare, Embarked,\n",
            "                          Title, Cabin_known, Family_size, Is_alone\n",
            "\n",
            "✗ Variables eliminades: PassengerId (identificador únic sense valor predictiu)\n",
            "                        Name (informació extreta en 'Title')\n",
            "                        Ticket (format inconsistent, difícil d'interpretar)\n",
            "                        Cabin (substituïda per 'Cabin_known')\n",
            "\n",
            "📊 TÈCNIQUES DE SELECCIÓ CONSIDERADES:\n",
            "\n",
            "• Mutual Information: Relació general entre features i target\n",
            "• Chi-quadrat / Cramér's V: Per variables categòriques\n",
            "• Point-biserial correlation: Per variables binàries\n",
            "• Spearman correlation: Per variables ordinals\n",
            "\n",
            "⚠️  NO APLICADES en aquesta fase perquè:\n",
            "   1. Alguns models (Random Forest, XGBoost) fan selecció implícita\n",
            "   2. Volem comparar el rendiment amb/sense totes les features\n",
            "   3. El cost computacional és acceptable amb aquest dataset\n",
            "\n",
            "💡 MILLORA FUTURA:\n",
            "   Avaluar la importància de cada feature en el model final (feature_importances_)\n",
            "   i eliminar aquelles amb contribució mínima, comparant l'accuracy resultant.\n",
            "\n",
            "🗑️  2.3 ELIMINACIÓ DE COLUMNES IRRELLEVANTS\n",
            "----------------------------------------\n",
            "Columnes eliminades: ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
            "\n",
            "Justificacions:\n",
            "  • PassengerId: Identificador únic sense valor predictiu\n",
            "  • Name: Informació extreta en 'Title'\n",
            "  • Ticket: Format inconsistent i difícil d'interpretar\n",
            "  • Cabin: Substituïda per 'Cabin_known'\n",
            "\n",
            "✓ Dataset resultant: (891, 12)\n",
            "\n",
            "🎯 2.4 SEPARACIÓ DE FEATURES I TARGET\n",
            "----------------------------------------\n",
            "✓ Features (X): (891, 11)\n",
            "✓ Target (y): (891,)\n",
            "\n",
            "✓ Distribució del target:\n",
            "   Classe 0: 61.62%\n",
            "   Classe 1: 38.38%\n",
            "\n",
            "📋 2.5 IDENTIFICACIÓ DE TIPUS DE VARIABLES\n",
            "----------------------------------------\n",
            "\n",
            "📊 Variables numèriques (5):\n",
            "   ['Age', 'Fare', 'SibSp', 'Parch', 'Family_size']\n",
            "\n",
            "🏷️  Variables categòriques (4):\n",
            "   ['Pclass', 'Sex', 'Embarked', 'Title']\n",
            "\n",
            "⚫⚪ Variables binàries (2):\n",
            "   ['Cabin_known', 'Is_alone']\n",
            "\n",
            "💡 ESTRATÈGIA DE CODIFICACIÓ:\n",
            "  • Numèriques: StandardScaler (després de imputar NaNs)\n",
            "  • Categòriques: OneHotEncoder amb drop='first' (evitar multicolinearitat)\n",
            "  • Binàries: Sense transformació (ja són 0/1)\n",
            "\n",
            "📐 2.6 ANÀLISI DE NORMALITZACIÓ\n",
            "----------------------------------------\n",
            "\n",
            "❓ ESTAN LES DADES NORMALITZADES?\n",
            "\n",
            "Estadístiques ABANS de normalitzar:\n",
            "            Age        Fare     SibSp     Parch  Family_size\n",
            "mean  29.699118   32.204208  0.523008  0.381594     1.904602\n",
            "std   14.526497   49.693429  1.102743  0.806057     1.613459\n",
            "min    0.420000    0.000000  0.000000  0.000000     1.000000\n",
            "max   80.000000  512.329200  8.000000  6.000000    11.000000\n",
            "\n",
            "❌ NO estan normalitzades:\n",
            "   • Age: rang [0.42, 80] amb mean≈29.7\n",
            "   • Fare: rang [0, 512] amb mean≈32.2\n",
            "   • Les escales són molt diferents → Afecta models basats en distàncies\n",
            "\n",
            "✅ CALDRIA NORMALITZAR-LES?\n",
            "\n",
            "   SÍ, pels següents motius:\n",
            "   1. Models sensibles a escales (KNN, SVM, Regressió Logística):\n",
            "      → Fare (0-512) dominaria sobre Age (0-80) en càlculs de distància\n",
            "   2. Millora la convergència de gradient descent\n",
            "   3. Facilita la interpretació de coeficients en models lineals\n",
            "   4. StandardScaler és robust a outliers moderats\n",
            "\n",
            "🔧 TIPUS DE NORMALITZACIÓ ESCOLLIDA: StandardScaler\n",
            "\n",
            "   Fórmula: z = (x - μ) / σ\n",
            "   • Transforma a mitjana=0 i std=1\n",
            "   • Mantén la distribució original\n",
            "   • No afectat per outliers extrems (a diferència de MinMaxScaler)\n",
            "\n",
            "   Alternatives considerades:\n",
            "   • MinMaxScaler: ✗ Sensible a outliers (Fare té valors extrems)\n",
            "   • RobustScaler: ✓ Alternativa vàlida, però StandardScaler és suficient\n",
            "   • Normalizer: ✗ Normalitza per files, no per columnes (no aplica aquí)\n",
            "\n",
            "🔬 2.7 CALDRIA APLICAR PCA?\n",
            "----------------------------------------\n",
            "\n",
            "❌ NO aplicarem PCA en aquest cas\n",
            "\n",
            "RAONS:\n",
            "  • Dataset petit (891 passatgers, ~15 features després de OneHot)\n",
            "  • No hi ha problema de dimensionalitat (regla general: n_samples >> n_features)\n",
            "  • Pèrdua d'interpretabilitat: no sabríem quines features són importants\n",
            "  • Les features tenen significat real i volem mantenir-lo\n",
            "\n",
            "✅ BENEFICIS de PCA (si s'apliqués):\n",
            "  + Reducció de dimensionalitat\n",
            "  + Eliminació de multicolinearitat\n",
            "  + Reducció de soroll\n",
            "  + Millora computacional en datasets grans\n",
            "\n",
            "❌ INCONVENIENTS de PCA:\n",
            "  - Pèrdua d'interpretabilitat (components principals no tenen significat clar)\n",
            "  - Assumeix relacions lineals\n",
            "  - Requereix normalització prèvia\n",
            "  - Pot eliminar informació rellevant per models no lineals\n",
            "\n",
            "💡 CONCLUSIÓ: Mantenim les features originals per interpretabilitat i\n",
            "              perquè el dataset no té problemes de dimensionalitat.\n",
            "\n",
            "🔄 2.8 CREACIÓ DEL PIPELINE DE PREPROCESSING\n",
            "----------------------------------------\n",
            "✓ Pipeline creat amb 3 components:\n",
            "\n",
            "  1️⃣  NUMÈRIQUES:\n",
            "      → KNNImputer(n_neighbors=5): Imputa Age basant-se en 5 veïns més propers\n",
            "      → StandardScaler(): Normalitza a mean=0, std=1\n",
            "      → Aplica a: ['Age', 'Fare', 'SibSp', 'Parch', 'Family_size']\n",
            "\n",
            "  2️⃣  CATEGÒRIQUES:\n",
            "      → SimpleImputer(most_frequent): Imputa Embarked amb la moda\n",
            "      → OneHotEncoder(drop='first'): Evita dummy variable trap\n",
            "      → Aplica a: ['Pclass', 'Sex', 'Embarked', 'Title']\n",
            "\n",
            "  3️⃣  BINÀRIES:\n",
            "      → Passthrough: No es transformen (ja són 0/1)\n",
            "      → Aplica a: ['Cabin_known', 'Is_alone']\n",
            "\n",
            "⚙️  2.9 TRAIN/TEST SPLIT I APLICACIÓ DEL PREPROCESSING\n",
            "----------------------------------------\n",
            "✓ Train set: (712, 11) (79.9%)\n",
            "✓ Test set:  (179, 11) (20.1%)\n",
            "\n",
            "✓ Distribució mantinguda (stratify=y):\n",
            "   Classe 0: Train=61.66%, Test=61.45%\n",
            "   Classe 1: Train=38.34%, Test=38.55%\n",
            "\n",
            "🔧 Aplicant preprocessing...\n",
            "\n",
            "✓ Dades preprocessades:\n",
            "   Train shape: (712, 16)\n",
            "   Test shape:  (179, 16)\n",
            "   Total features: 16\n",
            "\n",
            "✅ VERIFICACIÓ DE NORMALITZACIÓ (primeres 5 columnes numèriques):\n",
            "   Mitjana:     -0.000000 (esperat: ≈0)\n",
            "   Desv. std:   1.000000 (esperat: ≈1)\n",
            "   Min:         -2.21\n",
            "   Max:         10.01\n",
            "\n",
            "   ✓ Normalització correcta!\n",
            "\n",
            "================================================================================\n",
            "📝 RESUM DEL PREPROCESSING\n",
            "================================================================================\n",
            "\n",
            "✅ DECISIONS PRESES:\n",
            "   1. NaNs tractats amb KNNImputer (Age) i most_frequent (Embarked)\n",
            "   2. Feature engineering: Title, Cabin_known, Family_size, Is_alone\n",
            "   3. Normalització amb StandardScaler per variables numèriques\n",
            "   4. OneHotEncoder per variables categòriques\n",
            "   5. No s'aplica PCA (dataset petit, volem interpretabilitat)\n",
            "   6. Mantenim totes les features rellevants (no fem selecció agressiva)\n",
            "\n",
            "✅ DATASET FINAL:\n",
            "   Samples: 712 train + 179 test = 891 total\n",
            "   Features: 16 (després del preprocessing)\n",
            "   NaNs: 0 (tots imputats)\n",
            "   Normalitzat: Sí (variables numèriques)\n",
            "   Codificat: Sí (variables categòriques)\n",
            "\n",
            "✅ READY FOR MODEL TRAINING! 🚀\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#===================================================================================\n",
        "# APARTAT 2: PREPROCESSING (2 punts)\n",
        "#===================================================================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"APARTAT 2: PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# -------------------------\n",
        "# 2.1 Anàlisi de NaNs\n",
        "# -------------------------\n",
        "print(\"\\n📊 2.1 ANÀLISI DE VALORS FALTANTS\")\n",
        "print(\"-\" * 40)\n",
        "nan_info = pd.DataFrame({\n",
        "    'Total_NaN': df.isna().sum(),\n",
        "    'Percentatge': (df.isna().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "print(nan_info[nan_info['Total_NaN'] > 0])\n",
        "\n",
        "print(\"\\n⚠️  ESTRATÈGIA DE TRACTAMENT DE NaNs:\")\n",
        "print(\"\\n1. Age (19.87% NaNs):\")\n",
        "print(\"   → Estratègia: KNNImputer amb k=5\")\n",
        "print(\"   → Justificació: L'edat és una variable crítica per la supervivència.\")\n",
        "print(\"     KNNImputer utilitza la informació multivariable (Sex, Pclass, Fare, etc.)\")\n",
        "print(\"     per estimar valors més realistes que una simple mitjana.\")\n",
        "print(\"     Mantenim la variabilitat natural i les relacions amb altres variables.\")\n",
        "\n",
        "print(\"\\n2. Cabin (77.10% NaNs):\")\n",
        "print(\"   → Estratègia: Crear variable binària 'Cabin_known' + eliminar columna original\")\n",
        "print(\"   → Justificació: Massa NaNs per imputar de manera fiable.\")\n",
        "print(\"     Tenir cabina coneguda pot indicar classe social o proximitat a les sortides.\")\n",
        "print(\"     La informació binària (té/no té cabina) és més útil que la cabina específica.\")\n",
        "\n",
        "print(\"\\n3. Embarked (0.22% NaNs):\")\n",
        "print(\"   → Estratègia: SimpleImputer amb strategy='most_frequent'\")\n",
        "print(\"   → Justificació: Percentatge mínim de NaNs. La moda és suficient i no\")\n",
        "print(\"     distorsiona la distribució. És eficient i manté la consistència.\")\n",
        "\n",
        "print(\"\\n🔍 ALTERNATIVES CONSIDERADES I DESCARTADES:\")\n",
        "print(\"\\n• SimpleImputer (mean/median/mode):\")\n",
        "print(\"  ✗ Per Age: No considera relacions amb altres variables (Sex, Pclass, etc.)\")\n",
        "print(\"  ✓ Per Embarked: Adequat pel baix % de NaNs\")\n",
        "\n",
        "print(\"\\n• IterativeImputer (MICE):\")\n",
        "print(\"  ✗ Massa computacionalment costós per aquest dataset\")\n",
        "print(\"  ✗ Risc de sobreajust amb poques dades\")\n",
        "print(\"  ✗ KNNImputer ja captura relacions multivariables de forma més simple\")\n",
        "\n",
        "print(\"\\n• Eliminar files amb NaNs:\")\n",
        "print(\"  ✗ Perdríem ~20% del dataset (177 passatgers)\")\n",
        "print(\"  ✗ Pèrdua significativa d'informació per entrenar el model\")\n",
        "\n",
        "print(\"\\n• MissingIndicator:\")\n",
        "print(\"  ✗ Redundant: ja creem 'Cabin_known' per capturar aquesta informació\")\n",
        "\n",
        "print(\"\\n✓ CONCLUSIÓ: KNNImputer per Age + Cabin_known + most_frequent per Embarked\")\n",
        "\n",
        "# -------------------------\n",
        "# 2.2 Feature Engineering\n",
        "# -------------------------\n",
        "print(\"\\n🔧 2.2 FEATURE ENGINEERING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 2.2.1 Extreure títol del nom\n",
        "print(\"\\n📌 2.2.1 EXTRACCIÓ DE TÍTOL DEL NOM\")\n",
        "df['Title'] = df['Name'].str.extract(r' ([A-Za-z]+)\\.', expand=False)\n",
        "print(f\"Títols únics trobats: {df['Title'].nunique()}\")\n",
        "print(f\"\\nDistribució original de títols:\")\n",
        "print(df['Title'].value_counts())\n",
        "\n",
        "# Agrupar títols poc freqüents\n",
        "title_mapping = {\n",
        "    'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs',\n",
        "    'Lady': 'Mrs', 'Countess': 'Mrs', 'Dona': 'Mrs',\n",
        "    'Capt': 'Officer', 'Col': 'Officer', 'Major': 'Officer',\n",
        "    'Dr': 'Officer', 'Rev': 'Officer', 'Don': 'Officer',\n",
        "    'Sir': 'Officer', 'Jonkheer': 'Officer'\n",
        "}\n",
        "df['Title'] = df['Title'].replace(title_mapping)\n",
        "\n",
        "print(\"\\n✓ Títols agrupats:\")\n",
        "print(df['Title'].value_counts())\n",
        "print(\"\\nLògica d'agrupació:\")\n",
        "print(\"  • Miss: Dones joves/solteres (Miss, Mlle, Ms)\")\n",
        "print(\"  • Mrs: Dones casades/classe alta (Mrs, Mme, Lady, Countess, Dona)\")\n",
        "print(\"  • Officer: Homes amb títols oficials/militars/aristocràtics\")\n",
        "print(\"  • Mr, Master: Mantinguts per alta freqüència i rellevància\")\n",
        "\n",
        "# 2.2.2 Variable de cabina coneguda\n",
        "print(\"\\n📌 2.2.2 CREACIÓ DE CABIN_KNOWN\")\n",
        "df['Cabin_known'] = df['Cabin'].notna().astype(int)\n",
        "print(f\"✓ {df['Cabin_known'].sum()} passatgers ({df['Cabin_known'].mean()*100:.2f}%) amb cabina coneguda\")\n",
        "print(\"   Justificació: Tenir cabina pot correlacionar amb classe social i supervivència\")\n",
        "\n",
        "# 2.2.3 Mida de família\n",
        "print(\"\\n📌 2.2.3 CREACIÓ DE FAMILY_SIZE\")\n",
        "df['Family_size'] = df['SibSp'] + df['Parch'] + 1\n",
        "print(f\"✓ Rang de Family_size: [{df['Family_size'].min()}, {df['Family_size'].max()}]\")\n",
        "print(f\"   Distribució: {df['Family_size'].value_counts().sort_index().to_dict()}\")\n",
        "print(\"   Justificació: Famílies grans poden tenir més dificultat per evacuar\")\n",
        "\n",
        "# 2.2.4 Viatja sol\n",
        "print(\"\\n📌 2.2.4 CREACIÓ DE IS_ALONE\")\n",
        "df['Is_alone'] = (df['Family_size'] == 1).astype(int)\n",
        "print(f\"✓ {df['Is_alone'].sum()} passatgers ({df['Is_alone'].mean()*100:.2f}%) viatjant sols\")\n",
        "print(\"   Justificació: Viatjar sol pot afectar les possibilitats de supervivència\")\n",
        "\n",
        "# 2.2.5 Discussió sobre selecció de features\n",
        "print(\"\\n📌 2.2.5 SELECCIÓ DE FEATURES - ANÀLISI\")\n",
        "print(\"\\n🔍 ENFOCAMENT: Mantenir totes les variables (excepte les eliminades explícitament)\")\n",
        "print(\"\\nRAONAMENT:\")\n",
        "print(\"  • En ML no necessitem causalitat, sinó CORRELACIÓ/ASSOCIACIÓ\")\n",
        "print(\"  • Prioritzem ACCURACY sobre eficiència computacional\")\n",
        "print(\"  • Sense conèixer la naturalesa exacta de totes les dades, és arriscat eliminar-les\")\n",
        "print(\"\\n✓ Variables mantingudes: Pclass, Sex, Age, SibSp, Parch, Fare, Embarked,\")\n",
        "print(\"                          Title, Cabin_known, Family_size, Is_alone\")\n",
        "print(\"\\n✗ Variables eliminades: PassengerId (identificador únic sense valor predictiu)\")\n",
        "print(\"                        Name (informació extreta en 'Title')\")\n",
        "print(\"                        Ticket (format inconsistent, difícil d'interpretar)\")\n",
        "print(\"                        Cabin (substituïda per 'Cabin_known')\")\n",
        "\n",
        "print(\"\\n📊 TÈCNIQUES DE SELECCIÓ CONSIDERADES:\")\n",
        "print(\"\\n• Mutual Information: Relació general entre features i target\")\n",
        "print(\"• Chi-quadrat / Cramér's V: Per variables categòriques\")\n",
        "print(\"• Point-biserial correlation: Per variables binàries\")\n",
        "print(\"• Spearman correlation: Per variables ordinals\")\n",
        "print(\"\\n⚠️  NO APLICADES en aquesta fase perquè:\")\n",
        "print(\"   1. Alguns models (Random Forest, XGBoost) fan selecció implícita\")\n",
        "print(\"   2. Volem comparar el rendiment amb/sense totes les features\")\n",
        "print(\"   3. El cost computacional és acceptable amb aquest dataset\")\n",
        "\n",
        "print(\"\\n💡 MILLORA FUTURA:\")\n",
        "print(\"   Avaluar la importància de cada feature en el model final (feature_importances_)\")\n",
        "print(\"   i eliminar aquelles amb contribució mínima, comparant l'accuracy resultant.\")\n",
        "\n",
        "# -------------------------\n",
        "# 2.3 Eliminació de columnes\n",
        "# -------------------------\n",
        "print(\"\\n🗑️  2.3 ELIMINACIÓ DE COLUMNES IRRELLEVANTS\")\n",
        "print(\"-\" * 40)\n",
        "cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
        "print(f\"Columnes eliminades: {cols_to_drop}\")\n",
        "print(\"\\nJustificacions:\")\n",
        "print(\"  • PassengerId: Identificador únic sense valor predictiu\")\n",
        "print(\"  • Name: Informació extreta en 'Title'\")\n",
        "print(\"  • Ticket: Format inconsistent i difícil d'interpretar\")\n",
        "print(\"  • Cabin: Substituïda per 'Cabin_known'\")\n",
        "\n",
        "df_clean = df.drop(columns=cols_to_drop)\n",
        "print(f\"\\n✓ Dataset resultant: {df_clean.shape}\")\n",
        "\n",
        "# -------------------------\n",
        "# 2.4 Separació de features i target\n",
        "# -------------------------\n",
        "print(\"\\n🎯 2.4 SEPARACIÓ DE FEATURES I TARGET\")\n",
        "print(\"-\" * 40)\n",
        "X = df_clean.drop('Survived', axis=1)\n",
        "y = df_clean['Survived']\n",
        "print(f\"✓ Features (X): {X.shape}\")\n",
        "print(f\"✓ Target (y): {y.shape}\")\n",
        "print(f\"\\n✓ Distribució del target:\")\n",
        "distribucio = y.value_counts(normalize=True)\n",
        "for classe, percentatge in distribucio.items():\n",
        "    print(f\"   Classe {classe}: {percentatge:.2%}\")\n",
        "\n",
        "# -------------------------\n",
        "# 2.5 Identificació de tipus de variables\n",
        "# -------------------------\n",
        "print(\"\\n📋 2.5 IDENTIFICACIÓ DE TIPUS DE VARIABLES\")\n",
        "print(\"-\" * 40)\n",
        "numerical_features = ['Age', 'Fare', 'SibSp', 'Parch', 'Family_size']\n",
        "categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title']\n",
        "binary_features = ['Cabin_known', 'Is_alone']\n",
        "\n",
        "print(f\"\\n📊 Variables numèriques ({len(numerical_features)}):\")\n",
        "print(f\"   {numerical_features}\")\n",
        "print(f\"\\n🏷️  Variables categòriques ({len(categorical_features)}):\")\n",
        "print(f\"   {categorical_features}\")\n",
        "print(f\"\\n⚫⚪ Variables binàries ({len(binary_features)}):\")\n",
        "print(f\"   {binary_features}\")\n",
        "\n",
        "print(\"\\n💡 ESTRATÈGIA DE CODIFICACIÓ:\")\n",
        "print(\"  • Numèriques: StandardScaler (després d'imputar NaNs)\")\n",
        "print(\"  • Categòriques: OneHotEncoder amb drop='first' (evitar multicolinearitat)\")\n",
        "print(\"  • Binàries: Sense transformació (ja són 0/1)\")\n",
        "\n",
        "# -------------------------\n",
        "# 2.6 Anàlisi de normalització\n",
        "# -------------------------\n",
        "print(\"\\n📐 2.6 ANÀLISI DE NORMALITZACIÓ\")\n",
        "print(\"-\" * 40)\n",
        "print(\"\\n❓ ESTAN LES DADES NORMALITZADES?\")\n",
        "print(\"\\nEstadístiques ABANS de normalitzar:\")\n",
        "stats_before = X[numerical_features].describe().loc[['mean', 'std', 'min', 'max']]\n",
        "print(stats_before)\n",
        "\n",
        "print(\"\\n❌ NO estan normalitzades:\")\n",
        "print(f\"   • Age: rang [0.42, 80] amb mean≈{X['Age'].mean():.1f}\")\n",
        "print(f\"   • Fare: rang [0, 512] amb mean≈{X['Fare'].mean():.1f}\")\n",
        "print(f\"   • Les escales són molt diferents → Afecta models basats en distàncies\")\n",
        "\n",
        "print(\"\\n✅ CALDRIA NORMALITZAR-LES?\")\n",
        "print(\"\\n   SÍ, pels següents motius:\")\n",
        "print(\"   1. Models sensibles a escales (KNN, SVM, Regressió Logística):\")\n",
        "print(\"      → Fare (0-512) dominaria sobre Age (0-80) en càlculs de distància\")\n",
        "print(\"   2. Millora la convergència de gradient descent\")\n",
        "print(\"   3. Facilita la interpretació de coeficients en models lineals\")\n",
        "print(\"   4. StandardScaler és robust a outliers moderats\")\n",
        "\n",
        "print(\"\\n🔧 TIPUS DE NORMALITZACIÓ ESCOLLIDA: StandardScaler\")\n",
        "print(\"\\n   Fórmula: z = (x - μ) / σ\")\n",
        "print(\"   • Transforma a mitjana=0 i std=1\")\n",
        "print(\"   • Mantén la distribució original\")\n",
        "print(\"   • No afectat per outliers extrems (a diferència de MinMaxScaler)\")\n",
        "print(\"\\n   Alternatives considerades:\")\n",
        "print(\"   • MinMaxScaler: ✗ Sensible a outliers (Fare té valors extrems)\")\n",
        "print(\"   • RobustScaler: ✓ Alternativa vàlida (tractament d'outliers), però StandardScaler és suficient\")\n",
        "print(\"   • Normalizer: ✗ Normalitza per files, no per columnes (no aplica aquí)\")\n",
        "\n",
        "# -------------------------\n",
        "# 2.7 Discussió sobre PCA\n",
        "# -------------------------\n",
        "print(\"\\n🔬 2.7 CALDRIA APLICAR PCA?\")\n",
        "print(\"-\" * 40)\n",
        "print(\"\\n❌ NO aplicarem PCA en aquest cas\")\n",
        "print(\"\\nRAONS:\")\n",
        "print(\"  • Dataset petit (891 passatgers, ~15 features després de OneHot)\")\n",
        "print(\"  • No hi ha problema de dimensionalitat (regla general: n_samples >> n_features)\")\n",
        "print(\"  • Pèrdua d'interpretabilitat: no sabríem quines features són importants\")\n",
        "print(\"  • Les features tenen significat real i volem mantenir-lo\")\n",
        "\n",
        "print(\"\\n✅ BENEFICIS de PCA (si s'apliqués):\")\n",
        "print(\"  + Reducció de dimensionalitat\")\n",
        "print(\"  + Eliminació de multicolinearitat\")\n",
        "print(\"  + Reducció de soroll\")\n",
        "print(\"  + Millora computacional en datasets grans\")\n",
        "\n",
        "print(\"\\n❌ INCONVENIENTS de PCA:\")\n",
        "print(\"  - Pèrdua d'interpretabilitat (components principals no tenen significat clar)\")\n",
        "print(\"  - Assumeix relacions lineals\")\n",
        "print(\"  - Requereix normalització prèvia\")\n",
        "print(\"  - Pot eliminar informació rellevant per models no lineals\")\n",
        "\n",
        "print(\"\\n💡 CONCLUSIÓ: Mantenim les features originals per interpretabilitat i\")\n",
        "print(\"              perquè el dataset no té problemes de dimensionalitat.\")\n",
        "\n",
        "# -------------------------\n",
        "# 2.8 Pipeline de preprocessing\n",
        "# -------------------------\n",
        "print(\"\\n🔄 2.8 CREACIÓ DEL PIPELINE DE PREPROCESSING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Pipeline per variables numèriques: KNNImputer + StandardScaler\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', KNNImputer(n_neighbors=5)),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline per variables categòriques: SimpleImputer + OneHotEncoder\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# ColumnTransformer per combinar tots els transformadors\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'  # Variables binàries sense transformar\n",
        ")\n",
        "\n",
        "print(\"✓ Pipeline creat amb 3 components:\")\n",
        "print(\"\\n  1️⃣  NUMÈRIQUES:\")\n",
        "print(\"      → KNNImputer(n_neighbors=5): Imputa Age basant-se en 5 veïns més propers\")\n",
        "print(\"      → StandardScaler(): Normalitza a mean=0, std=1\")\n",
        "print(f\"      → Aplica a: {numerical_features}\")\n",
        "\n",
        "print(\"\\n  2️⃣  CATEGÒRIQUES:\")\n",
        "print(\"      → SimpleImputer(most_frequent): Imputa Embarked amb la moda\")\n",
        "print(\"      → OneHotEncoder(drop='first'): Evita dummy variable trap\")\n",
        "print(f\"      → Aplica a: {categorical_features}\")\n",
        "\n",
        "print(\"\\n  3️⃣  BINÀRIES:\")\n",
        "print(\"      → Passthrough: No es transformen (ja són 0/1)\")\n",
        "print(f\"      → Aplica a: {binary_features}\")\n",
        "\n",
        "# -------------------------\n",
        "# 2.9 Split i aplicació del preprocessing\n",
        "# -------------------------\n",
        "print(\"\\n⚙️  2.9 TRAIN/TEST SPLIT I APLICACIÓ DEL PREPROCESSING\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Split ABANS del preprocessing per evitar data leakage\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"✓ Train set: {X_train.shape} ({len(X_train)/len(X)*100:.1f}%)\")\n",
        "print(f\"✓ Test set:  {X_test.shape} ({len(X_test)/len(X)*100:.1f}%)\")\n",
        "\n",
        "print(\"\\n✓ Distribució mantinguda (stratify=y):\")\n",
        "dist_train = y_train.value_counts(normalize=True)\n",
        "dist_test = y_test.value_counts(normalize=True)\n",
        "for classe in [0, 1]:\n",
        "    print(f\"   Classe {classe}: Train={dist_train[classe]:.2%}, Test={dist_test[classe]:.2%}\")\n",
        "\n",
        "# Aplicar preprocessing\n",
        "print(\"\\n🔧 Aplicant preprocessing...\")\n",
        "X_train_preprocessed = preprocessor.fit_transform(X_train)\n",
        "X_test_preprocessed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"\\n✓ Dades preprocessades:\")\n",
        "print(f\"   Train shape: {X_train_preprocessed.shape}\")\n",
        "print(f\"   Test shape:  {X_test_preprocessed.shape}\")\n",
        "print(f\"   Total features: {X_train_preprocessed.shape[1]}\")\n",
        "\n",
        "# Verificar normalització\n",
        "n_numeric = len(numerical_features)\n",
        "sample_numeric = X_train_preprocessed[:, :n_numeric]\n",
        "print(f\"\\n✅ VERIFICACIÓ DE NORMALITZACIÓ (primeres {n_numeric} columnes numèriques):\")\n",
        "print(f\"   Mitjana:     {sample_numeric.mean():.6f} (esperat: ≈0)\")\n",
        "print(f\"   Desv. std:   {sample_numeric.std():.6f} (esperat: ≈1)\")\n",
        "print(f\"   Min:         {sample_numeric.min():.2f}\")\n",
        "print(f\"   Max:         {sample_numeric.max():.2f}\")\n",
        "\n",
        "if abs(sample_numeric.mean()) < 0.01 and abs(sample_numeric.std() - 1) < 0.1:\n",
        "    print(\"\\n   ✓ Normalització correcta!\")\n",
        "else:\n",
        "    print(\"\\n   ⚠️  Possible problema amb la normalització\")\n",
        "\n",
        "# -------------------------\n",
        "# 2.10 Resum final\n",
        "# -------------------------\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"📝 RESUM DEL PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n✅ DECISIONS PRESES:\")\n",
        "print(\"   1. NaNs tractats amb KNNImputer (Age) i most_frequent (Embarked)\")\n",
        "print(\"   2. Feature engineering: Title, Cabin_known, Family_size, Is_alone\")\n",
        "print(\"   3. Normalització amb StandardScaler per variables numèriques\")\n",
        "print(\"   4. OneHotEncoder per variables categòriques\")\n",
        "print(\"   5. No s'aplica PCA (dataset petit, volem interpretabilitat)\")\n",
        "print(\"   6. Mantenim totes les features rellevants (no fem selecció agressiva)\")\n",
        "print(\"\\n✅ DATASET FINAL:\")\n",
        "print(f\"   Samples: {len(X_train)} train + {len(X_test)} test = {len(X)} total\")\n",
        "print(f\"   Features: {X_train_preprocessed.shape[1]} (després del preprocessing)\")\n",
        "print(f\"   NaNs: 0 (tots imputats)\")\n",
        "print(f\"   Normalitzat: Sí (variables numèriques)\")\n",
        "print(f\"   Codificat: Sí (variables categòriques)\")\n",
        "print(\"\\n✅ READY FOR MODEL TRAINING! 🚀\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axc-wn2rCl7T"
      },
      "source": [
        "### 3. Metric selection (1.5 punts)\n",
        "En aquest apartat ens centrarem en les mètriques de classificació ([documentació](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)). Per a fer-ho, entreneu una regressio logistica i a partir d'aquesta generarem una serie de funcions per analitzar els nostres resultats. Aquestes funcions ens serviran més endavant. Caldrà tambe triar la mètrica que farem servir després per triar el millor model.\n",
        "\n",
        "**Preguntes:**\n",
        "* A teoria, hem vist el resultat d'aplicar el `accuracy_score` sobre dades no balancejades. Podrieu explicar i justificar quina de les següents mètriques será la més adient pel vostre problema? `accuracy_score`, `f1_score` o `average_precision_score`?\n",
        "* Abans de començar a entrenar models, genereu una suite de funcions per poder analitzar graficament com esta anant el vostre model. Mostreu la Precisió-Recall Curve i la ROC Curve. Quina és més rellevant pel vostre dataset?\n",
        "* Què mostra [classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)? Quina mètrica us fixareu per tal de optimitzar-ne la classificació pel vostre cas?\n",
        "\n",
        "**Nota**: Fixeu-vos que en aquest apartat NO ES VALOREN ELS RESULTATS. L'únic que es valora és l'elecció de la mètrica de classificació així com saber quin tipus de gràfiques fer per analitzar els resultats. Abans de solucionar un problema cal tenir molt clar la mètrica d'error que es farà servir, i és una decisió que cal pendre de forma prèvia a entrenar models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8xe5r78Cl7T"
      },
      "source": [
        "### 4. Model Selection amb validació creuada (4 punts)\n",
        "\n",
        "Fent servir la mètrica trobada en l'apartat anterior, en aquest apartat caldrà seleccionar una sèrie de models i, fent ús de la validació creuada, seleccionar el millor model amb els seus respectius millors hyperpàrametres que haurem buscat fent hyperparameter search.\n",
        "\n",
        "La tasca d'aquesta pràctica s'enmarca dins l'aprenentatge computacional **supervisat**. A sklearn, disposem de diverses tècniques [(veure documentació)](https://scikit-learn.org/stable/supervised_learning.html). A les classes de teoria, hem vist tècniques com ara logistic regression, SVM amb diferents kernels, Nearest Neighbour... i també coneixeu altres tecniques d'altres cursos, com els arbres de decisio. Per aquest apartat es demana seleccionar **un minim de 3 models**.\n",
        "\n",
        "**Preguntes:**\n",
        "* Quins models heu considerat? Per què els heu seleccionat?\n",
        "* Fent servir validació creuada, escolliu el millor model (agafant els hiperparàmetres per defecte). Recordeu fer servir la mètrica utilitzada en l'apartat anterior i fer fer servir algun [tipus de validacio creuada](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
        "\n",
        "* Seleccioneu una sèrie d'hiperparàmetres a provar per cadascun dels models i realitzeu una cerca d'hiperparàmetres. Hi ha algun model que creieu que podeu descartar de primeres? Per què?\n",
        "\n",
        "* Mostreu els resultats en una taula on es mostri el model, els experiments realitzats i els resultats obtinguts (tant en train com en test). Podeu mostrar tambe el temps d'entrenament de cada model.\n",
        "\n",
        "* Quin tipus de validació heu escollit en la selecció de models?\n",
        "\n",
        "* Quines formes de buscar els millors hiperparàmetres heu trobat? Són costoses computacionalment parlant? [documentació](https://scikit-learn.org/stable/modules/grid_search.html). Quina heu seleccionat?\n",
        "\n",
        "* Si disposem de recursos limitats (per exemple, un PC durant 1 hora), quin dels mètodes creieu que obtindrà millor resultat final?\n",
        "\n",
        "* Opcional: Feu la prova, i amb el model i el metode de validació creuada escollit, configureu els diferents mètodes de cerca per a que s'executin durant el mateix temps (i.e. depenent del problema, 0,5h-1 hora). Analitzeu quin ha arribat a una millor solució. (Ajuda: estimeu el temps que trigarà a fer 1 training el vostre model, i així trobeu el número de intents que podeu fer en cada cas.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hx-3b7v2TwJ3"
      },
      "source": [
        "### 5.Anàlisi Final (1.5 punt)\n",
        "\n",
        "Un cop seleccionat el millor model amb els millors hiperparàmetres, caldrà fer un anàlisi final amb els resultats obtinguts.\n",
        "\n",
        "**Preguntes:**\n",
        "* Mostreu les corves ROC/PR (la que hagueu escollit en l'apartat 2) i interpreteu els resultats.\n",
        "\n",
        "* Analitzeu en detall les diferents mètriques que trobeu adients i comenteu per sobre com podrieu fer servir aquest model en un futur. Això és el que es coneix com un cas d'ús.\n",
        "\n",
        "* Com creieu que es podria millorar el vostre model?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
